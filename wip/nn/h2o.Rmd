---
title: "neural nets/ deep learning with h2o and rsparkling"
author: "Dr. Shirin Glander"
date: '`r Sys.Date()`'
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

H2O is fast, scalable, open-source machine learning and deep learning for Smarter Applications. With H2O, enterprises like PayPal, Nielsen Catalina, Cisco and others can use all of their data without sampling and get accurate predictions faster. Advanced algorithms, like Deep Learning, Boosting, and Bagging Ensembles are readily available for application designers to build smarter applications through elegant APIs. Some of our earliest customers have built powerful domain-specific predictive engines for Recommendations, Customer Churn, Propensity to Buy, Dynamic Pricing and Fraud Detection for the Insurance, Healthcare, Telecommunications, AdTech, Retail and Payment Systems.

Using in-memory compression techniques, H2O can handle billions of data rows in-memory, even with a fairly small cluster. The platform includes interfaces for R, Python, Scala, Java, JSON and Coffeescript/JavaScript, along with a built-in web interface, Flow, that make it easier for non-engineers to stitch together complete analytic workflows. The platform was built alongside (and on top of) both Hadoop and Spark Clusters and is typically deployed within minutes.

H2O implements almost all common machine learning algorithms, such as generalized linear modeling (linear regression, logistic regression, etc.), Naïve Bayes, principal components analysis, time series, k-means clustering, and others. H2O also implements best-in-class algorithms such as Random Forest, Gradient Boosting, and Deep Learning at scale. Customers can build thousands of models and compare them to get the best prediction results.

H2O is nurturing a grassroots movement of physicists, mathematicians, computer and data scientists to herald the new wave of discovery with data science. Academic researchers and Industrial data scientists collaborate closely with our team to make this possible. Stanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise the H2O team to build scalable machine learning algorithms. With 100s of meetups over the past two years, H2O has become a word-of-mouth phenomenon growing amongst the data community by a 100-fold and is now used by 12,000+ users, deployed in 2000+ corporations using R, Python, Hadoop and Spark.

Try it out

H2O offers an R package that can be installed from CRAN

- Sparkling Water (H2O) Machine Learning: http://spark.rstudio.com/h2o.html

The rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.

Together with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.

rsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.

A typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:

Perform SQL queries through the sparklyr dplyr interface,
Use the sdf_* and ft_* family of functions to generate new columns, or partition your data set,
Convert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,
Choose an appropriate H2O machine learning algorithm to model your data,
Inspect the quality of your model fit, and use it to make predictions with new data.

- H2O, Sparkling Water, Steam, & Deep Water Documentation: http://docs.h2o.ai/h2o/latest-stable/index.html

H2O is the world’s leading open source machine learning platform. H2O is used by over 70,000 data scientists and more than 8,000 organizations around the world.

- http://www.h2o.ai/h2o/

H2O makes it possible for anyone to easily apply machine learning and predictive analytics to solve today’s most challenging business problems. It intelligently combines unique features not currently found in other machine learning platforms including:
Best of Breed Open Source Technology – Enjoy the freedom that comes with big data science powered by open source technology. H2O was written from scratch in Java and seamlessly integrates with the most popular open source products like Apache Hadoop® and Spark™ to give customers the flexibility to solve their most challenging data problems.
Easy-to-use WebUI and Familiar Interfaces – Set up and get started quickly using either H2O’s intuitive web-based Flow graphical user interface or familiar programming environments like R, Python, Java, Scala, JSON, and through our powerful APIs. Models can be visually inspected during training, which is unique to H2O.
Data Agnostic Support for all Common Database and File Types – Easily explore and model big data from within Microsoft Excel, R Studio, Tableau and more. Connect to data from HDFS, S3, SQL and NoSQL data sources. Install and deploy anywhere, in the cloud, on premise, on workstations, servers or clusters.
Massively Scalable Big Data Analysis – Train a model on complete data sets, not just small samples, and iterate and develop models in real-time with H2O’s rapid in-memory distributed parallel processing.
Real-time Data Scoring – Rapidly deploy models to production via plain-old Java objects (POJO) or model-optimized Java objects (MOJO). Score new data against models for accurate predictions in any environment. Enjoy faster scoring and better predictions than any other technology.

Combine the power of highly advanced algorithms, the freedom of open source, and the capacity of truly scalable in-memory processing for big data on one or many nodes. These capabilities make it faster, easier, and more cost effective to harness big data to maximum benefit for the business.
Data collection is easy. Decision making is hard. H2O makes it fast and easy to derive insights from your data through faster and better predictive modeling. Existing Big Data stacks are batch oriented. Search and analytics need to be interactive. Use machines to learn machine-generated data. And more data beats better algorithms.

With H2O, you can:

Make better predictions. Harness sophisticated, ready-to-use algorithms and the processing power you need to analyze bigger data sets, more models, and more variables.
Get started with minimal effort and investment. H2O is an extensible open source platform that offers the most pragmatic way to put big data to work for your business. With H2O, you can work with your existing languages and tools. Further, you can extend the platform seamlessly into your Hadoop environments.
Scalability + Speed
Fine-Grain Distributed Processing on Big Data at Speeds Up to 100x Faster.

Faster H2O lets you model interactively using in-memory processing, and delivers parallel distributed scalability required to support your big data production environments.

The solution combines the responsiveness of in-memory processing with the ability to run fast serialization between nodes and clusters—so you can support the size requirements of your large data sets. Further, H2O does this distributed processing with fine-grain parallelism, which enables optimal efficiency, without introducing degradation in computational accuracy.

In-Memory Processing Responsiveness

With H2O, your organization can harness the responsiveness of highly optimized in-memory processing, so you can operationalize many more models and gain real-time intelligence in business transactions and interactions.

With model export as plain old Java code, you gain lightning fast real-time scoring in any environment.

In addition, the solution enables data scientists to view partial query results while longer processes are running, so they can immediately spot a job that should be stopped and more quickly iterate to find the optimal approach.

- http://www.h2o.ai/deep-water/

H2O for GPU Enabled Deep Learning on All Data Types Integrating with TensorFlow, MXNet and Caffe

The recent advancements in open-source machine learning platforms and GPU technologies offer a solid foundation for Deep Learning to grow as one of the most prominent fields in Artificial Intelligence. Using complex multi-layer artificial neural networks, Deep Learning helps us derive insights from large unstructured data such as images, videos, sound and text or structured data from transactional databases such as financial data or time series. It enables innovations in domains as varied as medicine, social media, customer service, targeted marketing, automotive safety, security or fraud detection.

Open-source Deep Learning frameworks such as TensorFlow, MXNet and Caffe are optimized for fast training of such models using GPUs. GPUs excel at massively parallel workloads and speed up neural network training by 10-75x compared to conventional CPUs. Deep Water brings all these frameworks together under the same user interfaces as the H2O platform. Now, in addition to the original H2O Deep Learning algorithm, users can access TensorFlow, MXNet and Caffe backends in H2O, and build complex deep networks of up to 1,000’s of layers with GBs/TBs of data. Processing large datasets becomes orders of magnitude faster.

- http://www.h2o.ai/sparkling-water/

Sparkling Water

Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. With Sparkling Water, users can drive computation from Scala/R/Python and utilize the H2O Flow UI, providing an ideal machine learning platform for application developers.

Sparkling Water
What is Sparkling Water?

Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. With Sparkling Water, users can drive computation from Scala/R/Python and utilize the H2O Flow UI, providing an ideal machine learning platform for application developers.

What are the advantages of using Sparkling Water compared with H2O?

Sparkling Water contains the same features and functionality as H2O but provides a way to use H2O with Spark, a large-scale cluster framework.

Sparkling Water is ideal for H2O users who need to manage large clusters for their data processing needs and want to transfer data from Spark to H2O (or vice versa).

There is also a Python interface available to enable access to Sparkling Water directly from PySpark.

How do I filter an H2OFrame using Sparkling Water?

Filtering columns is easy: just remove the unnecessary columns or create a new H2OFrame from the columns you want to include (Frame(String[] names, Vec[] vec)), then make the H2OFrame wrapper around it (new H2OFrame(frame)).

Filtering rows is a little bit harder. There are two ways:

Create an additional binary vector holding 1/0 for the in/out sample (make sure to take this additional vector into account in your computations). This solution is quite cheap, since you do not duplicate data - just create a simple vector in a data walk.
or

Create a new frame with the filtered rows. This is a harder task, since you have to copy data. For reference, look at the #deepSlice call on Frame (H2OFrame)

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/SparklingWaterBooklet.pdf

The rsparkling R package is an extension package for sparklyr that creates an R front-end for the Sparkling Water Spark package from H2O. This provides an interface to H2O's high performance, distributed machine learning algorithms on Spark, using R.

This package implements basic functionality (creating an H2OContext, showing the H2O Flow interface, and converting between Spark DataFrames and H2O Frames). The main purpose of this package is to provide a connector between sparklyr and H2O's machine learning algorithms.

Install h2o

rsparkling currently requires that a certain version of H2O be used, depending on which major version of Spark is used, although this requirement will be relaxed in a future version. Each release of Sparking Water is built from specific versions of H2O, and those versions are listed in the table below.

rsparkling will automatically use the latest Sparkling Water based on the major Spark version provided. In this case, the H2O version required for the latest Sparkling Water is 3.10.3.2 for Spark 2.0 and 3.10.0.7 for Spark 1.6.

Advanced users may want to choose a particular Sparking Water / H2O version (specific Sparkling Water versions must match specific Spark and H2O versions), however any 2.0 version of Sparkling Water will work with any minor version of Spark 2.0. (similarly for 1.6). Refer to integration info below.

A Deep Neural Network (DNN) is an artificial neural network (ANN) with multiple hidden layers of units between the input and output layers. Similar to shallow ANNs, DNNs can model complex non-linear relationships. DNN architectures (e.g. for object detection and parsing) generate compositional models where the object is expressed as a layered composition of image primitives. The extra layers enable composition of features from lower layers, giving the potential of modeling complex data with fewer units than a similarly performing shallow network.

DNNs are typically designed as feedforward networks, but research has very successfully applied recurrent neural networks, especially LSTM, for applications such as language modeling. Convolutional deep neural networks (CNNs) are used in computer vision where their success is well-documented. CNNs also have been applied to acoustic modeling for automatic speech recognition, where they have shown success over previous models.

Deep Learning via Multilayer Perceptrons (MLPs)
One of the most common types of deep neural networks is the multilayer perceptron (MLP). From the deeplearningbook.org: “Deep feedforward networks, also often called feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models.”

Further, “These models are called feedforward because information ﬂows through the function being evaluated from xx, through the intermediate computations used to deﬁne ff, and ﬁnally to the output, yy. There are no feedback connections in which outputs of the model are fed back into itself. When feedforward neural networks are extended to include feedback connections, they are called recurrent neural networks.”

The h2o R package provides access to the H2O distributed Java-based implementation of a multilayer perceptron with many advanced features.



```{r eval=FALSE}
install.packages("rsparkling")
```

```{r message=FALSE, warning=FALSE}
library(rsparkling)
options(rsparkling.sparklingwater.version = "2.0.3")

library(h2o)
library(dplyr)
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.0.0")
```

```{r message=FALSE, warning=FALSE}
library(tidyr)
library(ggplot2)
library(ggrepel)

my_theme <- function(base_size = 12, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "aliceblue"),
    strip.background = element_rect(fill = "lightgrey", color = "grey", size = 1),
    strip.text = element_text(face = "bold", size = 12, color = "black"),
    legend.position = "right",
    legend.justification = "top", 
    panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )
}
```

https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names
https://archive.ics.uci.edu/ml/datasets/Arrhythmia

Class 01 refers to 'normal' ECG classes 02 to 15 refers to different classes of arrhythmia and class 16 refers to the rest of unclassified ones.

```{r}
arrhythmia <- read.table("arrhythmia.data.txt", sep = ",")
colnames(arrhythmia)[280] <- "class"
arrhythmia$class <- as.factor(arrhythmia$class)
summary(arrhythmia$class)
arrhythmia[-280] <- lapply(arrhythmia[-280], as.numeric)
```

```{r warning=FALSE, message=FALSE}
library(matrixStats)

colvars <- colVars(as.matrix(arrhythmia[-280]))
summary(colvars)
arrhythmia_subset <- arrhythmia[, c(280, which(colvars > 150))]
```

```{r}
arrhythmia_subset$class <- ifelse(arrhythmia_subset$class == 1, "normal", "arrhythmia")

arrhythmia_sc <- copy_to(sc, arrhythmia_subset, overwrite = TRUE)
arrhythmia_hf <- as_h2o_frame(sc, arrhythmia_sc, strict_version_check = FALSE)
```

```{r warning=FALSE, message=FALSE}
library(pcaGoPromoter)

pca_func <- function(pcaOutput2, group_name){
    centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)
    conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
          data.frame(groups = as.character(t),
                     ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                           centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                           level = 0.95),
                     stringsAsFactors = FALSE)))
        
    plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
      geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
      geom_point(size = 2, alpha = 0.6) + 
      scale_color_brewer(palette = "Set1") +
      scale_fill_brewer(palette = "Set1") +
      labs(color = paste(group_name),
           fill = paste(group_name),
           x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2) * 100, "% variance"),
           y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2) * 100, "% variance")) +
      my_theme()
    
    return(plot)
}
```

```{r eval=FALSE, echo=FALSE, fig.width=5, fig.height=4}
pcaOutput <- pca(t(arrhythmia_subset[-1]), printDropped = FALSE, scale = TRUE, center = TRUE)
pcaOutput2 <- as.data.frame(pcaOutput$scores)

pcaOutput2$groups <- arrhythmia_subset$class

pca_func(pcaOutput2, group_name = "")
```

```{r eval=FALSE, echo=FALSE}
partitions <- arrhythmia_sc %>%
  sdf_partition(training = 0.5, test = 0.5)

training <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
test <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)

summary(as.factor(training$class), exact_quantiles=TRUE)
summary(as.factor(test$class), exact_quantiles=TRUE)
```

```{r}
arrhythmia_hf[, 1] <- as.factor(arrhythmia_hf[, 1])

splits <- h2o.splitFrame(arrhythmia_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

response <- "class"
features <- setdiff(colnames(train), response)
```

```{r echo=FALSE, eval=FALSE}
h2o.hist(arrhythmia_hf$V1)
?h2o.interaction
```

```{r echo=FALSE, eval=FALSE}
rf_model <- h2o.randomForest(x = features, 
                             y = response,
                             training_frame = train,
                             validation_frame = valid,
                             model_id = "rf_model",
                             seed = 1,
                             nfolds = 5)

h2o.confusionMatrix(rf_model, valid = TRUE)

rf_perf <- h2o.performance(model = rf_model, newdata = test)
h2o.auc(rf_perf, xval = TRUE)

h2o.varimp_plot(rf_model)

h2o.varimp(rf_model)
```

```{r echo=FALSE, eval=FALSE}
gbm_fit3 <- h2o.gbm(x = features,
                    y = response,
                    training_frame = train,
                    model_id = "gbm_fit3",
                    validation_frame = valid,  #only used if stopping_rounds > 0
                    ntrees = 500,
                    score_tree_interval = 5,      #used for early stopping
                    stopping_rounds = 3,          #used for early stopping
                    stopping_metric = "AUC",      #used for early stopping
                    stopping_tolerance = 0.0005,  #used for early stopping
                    seed = 1)

gbm_perf3 <- h2o.performance(model = gbm_fit3,
                             newdata = test)

h2o.auc(gbm_perf3)

h2o.scoreHistory(gbm_fit3)

plot(gbm_fit3,
     timestep = "number_of_trees",
     metric = "AUC")
plot(gbm_fit3,
     timestep = "number_of_trees",
     metric = "logloss")

finalRf_predictions <- h2o.predict(
  object = gbm_fit3,
  newdata = test)
```

Early stopping (stop training before the specified number of epochs is completed to prevent overfitting) is enabled by default. If a validation frame is given, or if cross-validation is used, it will use validation error to determine the early stopping point. If just a training frame is given (and no CV), it will use the training set to perform early stopping.

Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data. To automatically find the optimal number of epochs, you must use H2O’s early stopping functionality. Unlike the rest of the H2O algorithms, H2O’s DL will use early stopping by default. We will use cross-validation (nfolds=3 to determine the optimal number of epochs. 

However, it may be more interesting to plot one (or all) of the CV models, as they will show the training error along with the validation error – a more informative plot with respect to evaluating overfitting. What you are seeing at the end of the plot is the validation and training error for the final (best) model.

Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. 

By default, H2O Deep Learning uses an adaptive learning rate ([ADADELTA](http://arxiv.org/pdf/1212.5701v1.pdf)) for its stochastic gradient descent optimization. There are only two tuning parameters for this method: `rho` and `epsilon`, which balance the global and local search efficiencies. `rho` is the similarity to prior weight updates (similar to momentum), and `epsilon` is a parameter that prevents the optimization to get stuck in local optima. Defaults are `rho=0.99` and `epsilon=1e-8`. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with `rho in c(0.9,0.95,0.99,0.999)` and `epsilon in c(1e-10,1e-8,1e-6,1e-4)`). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).

If `adaptive_rate` is disabled, several manual learning rate parameters become important: `rate`, `rate_annealing`, `rate_decay`, `momentum_start`, `momentum_ramp`, `momentum_stable` and `nesterov_accelerated_gradient`, the discussion of which we leave to [H2O Deep Learning booklet](http://h2o.ai/resources/).

```{r}
dl_model <- h2o.deeplearning(x = features,
                             y = response,
                             model_id = "dl_model",
                             training_frame = train,
                             validation_frame = valid,
                             hidden = c(200, 200, 200, 200), # 4 hidden layers, each of 200 neurons
                             nfolds = 10, # cross validation
                             seed = 1234)

summary(dl_model)

plot(dl_model)

h2o.confusionMatrix(dl_model, test)

perf <- h2o.performance(dl_model, test)

# Retreive test set MSE
h2o.mse(perf)

h2o.auc(perf, xval = TRUE)

finalRf_predictions <- h2o.predict(object = dl_model,
                                   newdata = test)

plot(dl_model,
     timestep = "epochs",
     metric = "classification_error")

# Get the CV models from the `dl_model` object
cv_models <- sapply(dl_model@model$cross_validation_models, function(i) h2o.getModel(i$name))

# Plot the scoring history over time
plot(cv_models[[1]],
     timestep = "epochs",
     metric = "classification_error")

plot(cv_models[[1]],
     timestep = "epochs",
     metric = "AUC")

plot(h2o.performance(dl_model))
```

### Hyper-parameter Tuning with Grid Search

Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning:

As an alternative to manual tuning, or “hand tuning”, we can use the h2o.grid() function to perform either a Cartesian or Random Grid Search (RGS). Random Grid Search is usually a quicker way to find a good model. The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search:

One handy feature of RGS is that you can specify how long you would like to execute the grid for – this can be based on a time, number of models, or a performance-metric-based stopping criterion.

First define a grid of Deep Learning hyperparameters and specify the search_criteria. Once we have trained the grid, we can collect the results and sort by our model performance metric of choice.

```{r}
activation_opt <- c("Rectifier", "Maxout", "Tanh")
l1_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01)
l2_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01)

hyper_params <- list(activation = activation_opt, 
                     l1 = l1_opt, 
                     l2 = l2_opt)

search_criteria <- list(strategy = "RandomDiscrete", max_runtime_secs = 60) # train grid for 60 seconds
```

```{r}
dl_grid <- h2o.grid("deeplearning", 
                    x = features,
                    y = response,
                    grid_id = "dl_grid",
                    training_frame = train,
                    validation_frame = valid,
                    seed = 1234,
                    hidden = c(200, 200, 200, 200),
                    nfolds = 10,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria)
```


```{r}
dl_gridperf <- h2o.getGrid(grid_id = "dl_grid",
                           sort_by = "accuracy",
                           decreasing = TRUE)

print(dl_gridperf)
```

Grab the model_id for the top DL model, chosen by validation error.

```{r}
best_dl_model_id <- dl_gridperf@model_ids[[1]]
best_dl <- h2o.getModel(best_dl_model_id)
```

Now let’s evaluate the model performance on a test set so we get an honest estimate of top model performance.

```{r}
best_dl_perf <- h2o.performance(model = best_dl, newdata = test)
h2o.mse(best_dl_perf)
```

Let's see which model had the lowest validation error:

```{r}
scores <- cbind(as.data.frame(unlist((lapply(grid@model_ids, function(x)
{ h2o.confusionMatrix(h2o.performance(h2o.getModel(x),valid=T))$Error[8] })) )), unlist(grid@model_ids))

names(scores) <- c("misclassification","model")
sorted_scores <- scores[order(scores$misclassification),]
head(sorted_scores)
best_model <- h2o.getModel(as.character(sorted_scores$model[1]))
print(best_model@allparameters)
best_err <- sorted_scores$misclassification[1]
print(best_err)
```

### Random Hyper-Parameter Search
Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build `N` models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters.

```{r}
models <- c()

for (i in 1:10) {

  rand_activation <- c("TanhWithDropout", "RectifierWithDropout")[sample(1:2, 1)]
  rand_numlayers <- sample(2:5,1)
  rand_hidden <- c(sample(10:50, rand_numlayers, T))
  rand_l1 <- runif(1, 0, 1e-3)
  rand_l2 <- runif(1, 0, 1e-3)
  rand_dropout <- c(runif(rand_numlayers, 0, 0.6))
  rand_input_dropout <- runif(1, 0, 0.5)
  
  dlmodel <- h2o.deeplearning(model_id = paste0("dl_random_model_", i),
                              training_frame = train,
                              validation_frame = valid,
                              x = features,
                              y = response,
                              nfolds = 10,
                              max_w2 = 10,                      ## can help improve stability for Rectifier
                              
                              ### Random parameters
                              activation = rand_activation,
                              hidden = rand_hidden,
                              l1  =rand_l1,
                              l2 = rand_l2,
                              input_dropout_ratio = rand_input_dropout,
                              hidden_dropout_ratios = rand_dropout)
  
  models <- c(models, dlmodel)
}
```

We continue to look for the model with the lowest validation misclassification rate:

```{r}
best_err <- 1      ##start with the best reference model from the grid search above, if available
for (i in 1:length(models)) {
  err <- h2o.confusionMatrix(h2o.performance(models[[i]],valid=T))$Error[8]
  if (err < best_err) {
    best_err <- err
    best_model <- models[[i]]
  }
}
h2o.confusionMatrix(best_model,valid=T)
best_params <- best_model@allparameters
best_params$hidden
best_params$l1
best_params$l2
best_params$input_dropout_ratio
```

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as `epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance` can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

```{r}
max_epochs <- 12 ## Add two more epochs
m_cont <- h2o.deeplearning(
model_id="dl_model_tuned_continued",
checkpoint="dl_model_tuned",
training_frame=train,
validation_frame=valid,
x=features,
y=response,
hidden=c(128,128,128),          ## more hidden layers -> more complex interactions
epochs=max_epochs,              ## hopefully long enough to converge (otherwise restart again)
stopping_metric="logloss",      ## logloss is directly optimized by Deep Learning
stopping_tolerance=1e-2,        ## stop when validation logloss does not improve by >=1% for 2 scoring events
stopping_rounds=2,
score_validation_samples=10000, ## downsample validation set for faster scoring
score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
adaptive_rate=F,                ## manually tuned learning rate
rate=0.01,
rate_annealing=2e-6,
momentum_start=0.2,             ## manually tuned momentum
momentum_stable=0.4,
momentum_ramp=1e7,
l1=1e-5,                        ## add some L1/L2 regularization
l2=1e-5,
max_w2=10                       ## helps stability for Rectifier
)
summary(m_cont)
plot(m_cont)
```

```{r}
# Grid Search

# GBM hyperparamters
gbm_params1 <- list(learn_rate = c(0.01, 0.1),
                    max_depth = c(3, 5, 9),
                    sample_rate = c(0.8, 1.0),
                    col_sample_rate = c(0.2, 0.5, 1.0))

# Train and validate a grid of GBMs
gbm_grid1 <- h2o.grid("gbm", 
                      x = features, 
                      y = response,
                      grid_id = "gbm_grid1",
                      training_frame = splits[[1]],
                      validation_frame = splits[[2]],
                      ntrees = 100,
                      seed = 1,
                      hyper_params = gbm_params1)

# Get the grid results, sorted by validation MSE
gbm_gridperf1 <- h2o.getGrid(grid_id = "gbm_grid1", 
                             sort_by = "mse", 
                             decreasing = FALSE)

print(gbm_gridperf1)
```

Once we are satisfied with the results, we can save the model to disk (on the cluster). In this example, we store the model in a directory called `mybest_deeplearning_covtype_model`, which will be created for us since `force=TRUE`.

```{r}
path <- h2o.saveModel(m_cont,
                      path="./mybest_deeplearning_covtype_model", force=TRUE)
```

It can be loaded later with the following command:

```{r}
print(path)
m_loaded <- h2o.loadModel(path)
summary(m_loaded)
```

This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. Note that binary compatibility between H2O versions is currently not guaranteed.

###Cross-Validation
For N-fold cross-validation, specify `nfolds>1` instead of (or in addition to) a validation frame, and `N+1` models will be built: 1 model on the full training data, and N models with each 1/N-th of the data held out (there are different holdout strategies). Those N models then score on the held out data, and their combined predictions on the full training data are scored to get the cross-validation metrics.

```{r}
dlmodel <- h2o.deeplearning(
  x=features,
  y=response,
  training_frame=train,
  hidden=c(10,10),
  epochs=1,
  nfolds=5,
  fold_assignment="Modulo" # can be "AUTO", "Modulo", "Random" or "Stratified"
)
dlmodel
```

N-fold cross-validation is especially useful with early stopping, as the main model will pick the ideal number of epochs from the convergence behavior of the cross-validation models.

##H2O Deep Learning Tips & Tricks
####Activation Functions
While sigmoids have been used historically for neural networks, H2O Deep Learning implements `Tanh`, a scaled and shifted variant of the sigmoid which is symmetric around 0. Since its output values are bounded by -1..1, the stability of the neural network is rarely endangered. However, the derivative of the tanh function is always non-zero and back-propagation (training) of the weights is more computationally expensive than for rectified linear units, or `Rectifier`, which is `max(0,x)` and has vanishing gradient for `x<=0`, leading to much faster training speed for large networks and is often the fastest path to accuracy on larger problems. In case you encounter instabilities with the `Rectifier` (in which case model building is automatically aborted), try a limited value to re-scale the weights: `max_w2=10`. The `Maxout` activation function is computationally more expensive, but can lead to higher accuracy. It is a generalized version of the Rectifier with two non-zero channels. In practice, the `Rectifier` (and `RectifierWithDropout`, see below) is the most versatile and performant option for most problems.

####Generalization Techniques
L1 and L2 penalties can be applied by specifying the `l1` and `l2` parameters. Intuition: L1 lets only strong weights survive (constant pulling force towards zero), while L2 prevents any single weight from getting too big. [Dropout](http://arxiv.org/pdf/1207.0580.pdf) has recently been introduced as a powerful generalization technique, and is available as a parameter per layer, including the input layer. `input_dropout_ratio` controls the amount of input layer neurons that are randomly dropped (set to zero), while `hidden_dropout_ratios` are specified for each hidden layer. The former controls overfitting with respect to the input data (useful for high-dimensional noisy data), while the latter controls overfitting of the learned features. Note that `hidden_dropout_ratios` require the activation function to end with `...WithDropout`.

####Early stopping and optimizing for lowest validation error
By default, Deep Learning training stops when the `stopping_metric` does not improve by at least `stopping_tolerance` (0.01 means 1% improvement) for `stopping_rounds` consecutive scoring events on the training (or validation) data. By default, `overwrite_with_best_model` is enabled and the model returned after training for the specified number of epochs (or after stopping early due to convergence) is the model that has the best training set error (according to the metric specified by `stopping_metric`), or, if a validation set is provided, the lowest validation set error. Note that the training or validation set errors can be based on a subset of the training or validation data, depending on the values for `score_validation_samples` or `score_training_samples`, see below. For early stopping on a predefined error rate on the *training data* (accuracy for classification or MSE for regression), specify `classification_stop` or `regression_stop`.

####Training Samples per MapReduce Iteration
The parameter `train_samples_per_iteration` matters especially in multi-node operation. It controls the number of rows trained on for each MapReduce iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of `-2` indicates auto-tuning, which attemps to keep the communication overhead at 5% of the total runtime. The parameter `target_ratio_comm_to_comp` controls this ratio. This parameter is explained in more detail in the [H2O Deep Learning booklet](http://h2o.ai/resources/),

####Categorical Data
For categorical data, a feature with K factor levels is automatically one-hot encoded (horizontalized) into K-1 input neurons. Hence, the input neuron layer can grow substantially for datasets with high factor counts. In these cases, it might make sense to reduce the number of hidden neurons in the first hidden layer, such that large numbers of factor levels can be handled. In the limit of 1 neuron in the first hidden layer, the resulting model is similar to logistic regression with stochastic gradient descent, except that for classification problems, there's still a softmax output layer, and that the activation function is not necessarily a sigmoid (`Tanh`). If variable importances are computed, it is recommended to turn on `use_all_factor_levels` (K input neurons for K levels). The experimental option `max_categorical_features` uses feature hashing to reduce the number of input neurons via the hash trick at the expense of hash collisions and reduced accuracy. Another way to reduce the dimensionality of the (categorical) features is to use `h2o.glrm()`, we refer to the GLRM tutorial for more details.

####Missing Values
H2O Deep Learning automatically does mean imputation for missing values during training (leaving the input layer activation at 0 after standardizing the values). For testing, missing test set values are also treated the same way by default. See the `h2o.impute` function to do your own mean imputation.

####Loss functions, Distributions, Offsets, Observation Weights
H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights.
In addition to `Gaussian` distributions and `Squared` loss, H2O Deep Learning supports `Poisson`, `Gamma`, `Tweedie` and `Laplace` distributions. It also supports `Absolute` and `Huber` loss and per-row offsets specified via an `offset_column`. Observation weights are supported via a user-specified `weights_column`.

We refer to our [H2O Deep Learning R test code examples](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/deeplearning) for more information.

####Exporting Weights and Biases
The model parameters (weights connecting two adjacent layers and per-neuron bias terms) can be stored as H2O Frames (like a dataset) by enabling `export_weights_and_biases`, and they can be accessed as follows:

```{r}
h2o.weights(m3, matrix_id=1)
h2o.weights(m3, matrix_id=2)
h2o.weights(m3, matrix_id=3)
h2o.biases(m3,  vector_id=1)
h2o.biases(m3,  vector_id=2)
h2o.biases(m3,  vector_id=3)
plot(as.data.frame(h2o.weights(m3,  matrix_id=1))[,1])
```

####Reproducibility
Every run of DeepLearning results in different results since multithreading is done via [Hogwild!](http://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf) that benefits from intentional lock-free race conditions between threads. To get reproducible results for small datasets and testing purposes, set `reproducible=T` and set `seed=1337` (pick any integer). This will not work for big data for technical reasons, and is probably also not desired because of the significant slowdown (runs on 1 core only).

####Scoring on Training/Validation Sets During Training
The training and/or validation set errors *can* be based on a subset of the training or validation data, depending on the values for `score_validation_samples` (defaults to 0: all) or `score_training_samples` (defaults to 10,000 rows, since the training error is only used for early stopping and monitoring). For large datasets, Deep Learning can automatically sample the validation set to avoid spending too much time in scoring during training, especially since scoring results are not currently displayed in the model returned to R.

Note that the default value of `score_duty_cycle=0.1` limits the amount of time spent in scoring to 10%, so a large number of scoring samples won't slow down overall training progress too much, but it will always score once after the first MapReduce iteration, and once at the end of training.

Stratified sampling of the validation dataset can help with scoring on datasets with class imbalance.  Note that this option also requires `balance_classes` to be enabled (used to over/under-sample the training dataset, based on the max. relative size of the resulting training dataset, `max_after_balance_size`):


####Deep Learning Autoencoders
Deep Learning Autoencoders can be used for both unsupervised pre-training of a supervised deep neural network or for anomaly detection. We will demonstrate these applications using the h2o package below.

From Statistical Learning with Sparsity (Hastie, Tibshirani, Wainwright, 2015) Section 8.2.5: “In the neural network literature, an autoencoder generalizes the idea of principal components.”

Autoencoders for Unsupervised Pre-Training
On sparse autoencoders (although this can be said of autoencoders in general):

“One important use of the sparse autoencoder is for pretraining. When fitting a supervised neural network to labelled data, it is often advantageous to first fit an autoencoder to the data without the labels and then use the resulting weights as starting values for fitting the supervised neural network (Erhan et al. 2010). Because the neural-network objective function is nonconvex, these starting weights can significantly improve the quality of the final solution. Furthermore, if there is additional data available without labels, the autoencoder can make use of these data in the pretraining phase.”

------------------

<br>

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4, fig.align="center", cache=FALSE}
sessionInfo()
```
