---
title: "Hyper-parameter Tuning with Grid Search for Deep Learning"
author: "Shirin Glander"
date: "February 21, 2017"
output: html_document
---

- link to last week's post

---

Last week I showed how to build a deep neural network with h2o and rsparkling. As we could see there, it is not trivial to optimize the hyper-parameters for modeling. Hyper-parameter tuning with grid search allows us to test different combinations of hyper-parameters and find one with improved accuracy.

Keep in mind though, that hyperparameter tuning can only improve the model so much without overfitting. If you can't achieve sufficient accuracy, the input features might simply not be adequate for the predictions you are trying to model. It might be necessary to go back to the original features and try e.g. feature engineering methods.

<br>

### Preparing Spark instance and plotting theme

Check out last week's post for details.

```{r message=FALSE, warning=FALSE, tidy=FALSE}
library(rsparkling)
options(rsparkling.sparklingwater.version = "2.0.3")

library(h2o)
library(dplyr)
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.0.0")
```

```{r message=FALSE, warning=FALSE, tidy=FALSE}
library(ggplot2)
library(ggrepel)

my_theme <- function(base_size = 12, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "aliceblue"),
    strip.background = element_rect(fill = "darkgrey", color = "grey", size = 1),
    strip.text = element_text(face = "bold", size = 12, color = "white"),
    legend.position = "right",
    legend.justification = "top", 
    panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )
}
```

```{r echo=FALSE}
load("arrhythmia_subset.RData")
```

```{r tidy=FALSE}
arrhythmia_sc <- copy_to(sc, arrhythmia_subset)
arrhythmia_hf <- as_h2o_frame(sc, arrhythmia_sc, strict_version_check = FALSE)
```

```{r tidy=FALSE}
splits <- h2o.splitFrame(arrhythmia_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

response <- "diagnosis"
weights <- "weights"
features <- setdiff(colnames(train), c(response, weights, "class"))
```

<br>

### Grid Search

We can use the *h2o.grid()* function to perform e.g. Random Grid Search (RGS). We could also test all possible combinations of parameters with Cartesian Grid Search, but RGS is much faster and usually finds sufficiently accurate models.

For RGS we first define a set of hyper-parameters and search criteria, the grid. We can also specify how long we want to run the grid search for. Based on the results of each model tested in the grid, we can choose the one with the highest accuracy or best performance for the question on hand.

#### Activation Functions
- **Rectifier**: is the default activation function. It is the fastest and most versatile option. It can lead to instabilities though and tends to be lower in accuracy.
- **Tanh**: ia a scaled and shifted variant of the sigmoid activation function. It can take on values from -1 to 1 and centers around 0. Tanh needs more computational power than e.g. the Rectifier function.
- **Maxout**: is computationally quite demanding but can produce high accuracy models.

- **...WithDropout**: works together with the parameter *hidden_dropout_ratios* (ontrols the amount of layer neurons that are randomly dropped for each hidden layer). Hidden dropout ratios are useful for preventing overfitting on learned features.

#### L1 and L2 penalties
- **L1**: lets only strong weights survive
- **L2**: prevents any single weight from getting too big. 

```{r}
hyper_params <- list(activation = c("Rectifier", "Maxout", "Tanh"), 
                     l1 = c(0, 0.00001, 0.0001, 0.001, 0.01), 
                     l2 = c(0, 0.00001, 0.0001, 0.001, 0.01))

hyper_params <- list(activation = c("RectifierWithDropout", "MaxoutWithDropout", "TanhWithDropout"), 
                     l1 = c(0, 0.00001, 0.0001, 0.001, 0.01), 
                     l2 = c(0, 0.00001, 0.0001, 0.001, 0.01),
                     hidden_dropout_ratios = c())
```

#### Early stopping criteria
- **stopping_metric**: metric that we want to use as stopping criterion
- **stopping_tolerance** and **stopping_rounds**: trainig stops when the the stopping metric does not improve by the stopping tolerance proportion any more (e.g. by 0.05 or 5%) for the number of consecutive rounds defined by stopping rounds.

```{r}
search_criteria <- list(strategy = "RandomDiscrete", 
                        max_models = 100,
                        max_runtime_secs = 300)

#search_criteria <- list(strategy = "RandomDiscrete", 
 #                       stopping_metric = "AUTO", 
  #                      stopping_tolerance = 0.001, 
   #                     stopping_rounds = 10)
```

```{r}
dl_grid <- h2o.grid(algorithm = "deeplearning", 
                    x = features,
                    y = response,
                    weights_column = weights,
                    model_id = "dl_grid",
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 15,                                   # 10x cross validation
                    keep_cross_validation_fold_assignment = TRUE,
                    score_each_iteration = TRUE,
                    hidden = c(200, 200, 200, 200, 200),           # 5 hidden layers, each of 200 neurons
                    epochs = 100,
                    variable_importances = TRUE,
                    export_weights_and_biases = TRUE,
                    seed = 42,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria)
```

```{r echo=TRUE, eval=FALSE}
h2o.saveModel(dl_grid, path = "dl_grid", force = TRUE)
```

```{r}
dl_grid <- h2o.loadModel("/Users/Shirin/Documents/Github/blog_posts_prep/wip/nn/dl_grid/dl_grid")
```

```{r}
grid <- h2o.getGrid(grid_id = "dl_grid",
                    sort_by = "accuracy",
                    decreasing = TRUE)
```

```{r}
model_ids <- grid@model_ids
best_model <- h2o.getModel(model_ids[[1]])
```

```{r }
h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)
```

Now letâ€™s evaluate the model performance on a test set so we get an honest estimate of top model performance.

```{r}
perf <- h2o.performance(best_model, test)
plot(perf)
h2o.mse(perf)
h2o.auc(perf)
h2o.confusionMatrix(best_model, test)
```



####Training Samples per MapReduce Iteration
The parameter `train_samples_per_iteration` matters especially in multi-node operation. It controls the number of rows trained on for each MapReduce iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of `-2` indicates auto-tuning, which attemps to keep the communication overhead at 5% of the total runtime. The parameter `target_ratio_comm_to_comp` controls this ratio. This parameter is explained in more detail in the [H2O Deep Learning booklet](http://h2o.ai/resources/),


####Loss functions, Distributions, Offsets, Observation Weights
H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights.
In addition to `Gaussian` distributions and `Squared` loss, H2O Deep Learning supports `Poisson`, `Gamma`, `Tweedie` and `Laplace` distributions. It also supports `Absolute` and `Huber` loss and per-row offsets specified via an `offset_column`. Observation weights are supported via a user-specified `weights_column`.


`rho` and `epsilon`, which balance the global and local search efficiencies. `rho` is the similarity to prior weight updates (similar to momentum), and `epsilon` is a parameter that prevents the optimization to get stuck in local optima. Defaults are `rho=0.99` and `epsilon=1e-8`. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with `rho in c(0.9,0.95,0.99,0.999)` and `epsilon in c(1e-10,1e-8,1e-6,1e-4)`). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).


In the context of artificial neural networks, the rectifier is an activation function defined as

{\displaystyle f(x)=\max(0,x),} {\displaystyle f(x)=\max(0,x),}
where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature[1] with strong biological motivations and mathematical justifications.[2] It has been used in convolutional networks[3] more effectively than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical[4] counterpart, the hyperbolic tangent. The rectifier is, as of 2015, the most popular activation function for deep neural networks.[5]

A unit employing the rectifier is also called a rectified linear unit (ReLU).[6]

A smooth approximation to the rectifier is the analytic function

{\displaystyle f(x)=\ln(1+e^{x}),} {\displaystyle f(x)=\ln(1+e^{x}),}
which is called the softplus function.[7] The derivative of softplus is {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})} {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})}, i.e. the logistic function.

Rectified linear units find applications in computer vision[3] and speech recognition[8][9] using deep neural nets.

A maxout layer is simply a layer where the activation function is the max of the inputs. As stated in the paper, even an MLP with 2 maxout units can approximate any function. They give a couple of reasons as to why maxout may be performing well, but the main reason they give is the following --

Dropout can be thought of as a form of model averaging in which a random subnetwork is trained at every iteration and in the end the weights of the different such random networks are averaged. Since one cannot average the weights explicitly, an approximation is used. This approximation is exact for for a linear network 
In maxout, they do not drop the inputs to the maxout layer. Thus the identity of the input outputting the max value for a data point remains unchanged. Thus the dropout only happens in the linear part of the MLP but one can still approximate any function because of the maxout layer. 
As the dropout happens in the linear part only, they conjecture that this leads to more efficient model averaging as the averaging approximation is exact for linear networks.

tanh: Hyperbolic tangent:
Although tanh is just a scaled and shifted version of a logistic sigmoid, one of the prime reasons why tanh is the preferred activation/transfer function is because it squashes to a wider numerical range (-1..1) and has asymptotic symmetry.

There are several implications to these properties,

It results in faster convergence than the standard logistic function.
On average, it's more likely to create output values that are close to 0, which is beneficial when forward propagating to subsequent layers.
Overall makes training less difficult as it less proned to saturation in the later layers of your network.
When supplying transformed or normalized inputs, the variance of the outputs will also be close to 1. This makes it great for when you're training on image corpora.
It also has a nice derivative (1-tanh^2)! For training purposes, you can calculate the value of the derivative just by using the output, no knowledge of the input is needed.
Yann Lecun's shares some great insight in his paper Efficient Backpropagation on why tanh works well in-practice.

From my own experience, I've found tanh particularly beneficial when used with deep-convolutional neural networks. The intuition behind choosing an ideal activation functions moreso depending on the nature of your problem, dataset, and type of network.

It's best to test it out yourself and compare the results emprically; there are many times where it won't make much of a difference at all (especially in shallow networks).

On the field of Artificial Neural Networks, the sigmoid funcion is a type of activation function for artifical neurons.

The most basic activation funciton is the Heaviside (binary step, 0 or 1, high or low):

You can have several types of activation functions and they are best suitable for different purposes. In the specific case of the Sigmoid:

Real-valued and differentiable (you need this to find gradients);
Analytic tractability for the differentiaton operation;
It is an acceptable mathematical representation of a biological neuron behaviour. The output shows if the neuron is firing or not.
There is a table with the various types of activation functions:

Activation function

Most of the time you will be concerned about the following points when choosing the activation function:

continuity of the function
computational power to process all neurons of the network
type of the desired output (logistic/continuous variables or classification/categorical data)




Let's see which model had the lowest validation error:

```{r}
scores <- cbind(as.data.frame(unlist((lapply(dl_grid@model_ids, function(x)
{ h2o.confusionMatrix(h2o.performance(h2o.getModel(x),valid=T))$Error[3] })) )), unlist(dl_grid@model_ids))

names(scores) <- c("misclassification","model")
sorted_scores <- scores[order(scores$misclassification),]
head(sorted_scores)
best_model <- h2o.getModel(as.character(sorted_scores$model[1]))
print(best_model@allparameters)
best_err <- sorted_scores$misclassification[1]
print(best_err)
```

### Random Hyper-Parameter Search

Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build `N` models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters.

```{r}
models <- c()

for (i in 1:10) {

  rand_activation <- c("TanhWithDropout", "RectifierWithDropout")[sample(1:2, 1)]
  rand_numlayers <- sample(2:5,1)
  rand_hidden <- c(sample(10:50, rand_numlayers, T))
  rand_l1 <- runif(1, 0, 1e-3)
  rand_l2 <- runif(1, 0, 1e-3)
  rand_dropout <- c(runif(rand_numlayers, 0, 0.6))
  rand_input_dropout <- runif(1, 0, 0.5)
  
  dlmodel <- h2o.deeplearning(model_id = paste0("dl_random_model_", i),
                              training_frame = train,
                              validation_frame = valid,
                              x = features,
                              y = response,
                              nfolds = 10,
                              max_w2 = 10,                      ## can help improve stability for Rectifier
                              
                              ### Random parameters
                              activation = rand_activation,
                              hidden = rand_hidden,
                              l1 = rand_l1,
                              l2 = rand_l2,
                              input_dropout_ratio = rand_input_dropout,
                              hidden_dropout_ratios = rand_dropout)
  
  models <- c(models, dlmodel)
}
```

We continue to look for the model with the lowest validation misclassification rate:

```{r}
best_err <- 1
for (i in 1:length(models)) {
  err <- h2o.confusionMatrix(h2o.performance(models[[i]],valid=T))$Error[3]
  if (err < best_err) {
    best_err <- err
    best_model <- models[[i]]
  }
}
h2o.confusionMatrix(best_model,valid=T)
best_params <- best_model@allparameters
best_params$hidden
best_params$l1
best_params$l2
best_params$input_dropout_ratio
```

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as `epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance` can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

```{r}
max_epochs <- 12 ## Add two more epochs
m_cont <- h2o.deeplearning(model_id="dl_model_tuned_continued",
                          training_frame=train,
                          validation_frame=valid,
                          x=features,
                          y=response,
                          hidden=c(128,128,128),          ## more hidden layers -> more complex interactions
                          epochs=max_epochs,              ## hopefully long enough to converge (otherwise restart again)
                          stopping_metric="logloss",      ## logloss is directly optimized by Deep Learning
                          stopping_tolerance=1e-2,        ## stop when validation logloss does not improve by >=1% for 2 scoring events
                          stopping_rounds=2,
                          score_validation_samples=10000, ## downsample validation set for faster scoring
                          score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
                          adaptive_rate=F,                ## manually tuned learning rate
                          rate=0.01,
                          rate_annealing=2e-6,
                          momentum_start=0.2,             ## manually tuned momentum
                          momentum_stable=0.4,
                          momentum_ramp=1e7,
                          l1=1e-5,                        ## add some L1/L2 regularization
                          l2=1e-5,
                          max_w2=10                       ## helps stability for Rectifier
)
summary(m_cont)
plot(m_cont)
```

