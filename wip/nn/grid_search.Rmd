---
title: "Hyper-parameter Tuning with Grid Search for Deep Learning"
author: "Shirin Glander"
date: "February 21, 2017"
output: html_document
---

- link to last week's post

---

Last week I showed how to build a deep neural network with h2o and rsparkling. As we could see there, it is not trivial to optimize the hyper-parameters for modeling. Hyper-parameter tuning with grid search allows us to test different combinations of hyper-parameters and find one with improved accuracy.

Keep in mind though, that hyperparameter tuning can only improve the model so much without overfitting. If you can't achieve sufficient accuracy, the input features might simply not be adequate for the predictions you are trying to model. It might be necessary to go back to the original features and try e.g. feature engineering methods.

<br>

### Preparing Spark instance and plotting theme

Check out last week's post for details.

```{r message=FALSE, warning=FALSE, tidy=FALSE}
library(rsparkling)
options(rsparkling.sparklingwater.version = "2.0.3")

library(h2o)
library(dplyr)
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.0.0")
```

```{r message=FALSE, warning=FALSE, tidy=FALSE}
library(ggplot2)
library(ggrepel)

my_theme <- function(base_size = 12, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "aliceblue"),
    strip.background = element_rect(fill = "darkgrey", color = "grey", size = 1),
    strip.text = element_text(face = "bold", size = 12, color = "white"),
    legend.position = "right",
    legend.justification = "top", 
    panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )
}
```

```{r echo=FALSE}
load("arrhythmia_subset.RData")
```

```{r tidy=FALSE}
arrhythmia_sc <- copy_to(sc, arrhythmia_subset)
arrhythmia_hf <- as_h2o_frame(sc, arrhythmia_sc, strict_version_check = FALSE)
```

```{r tidy=FALSE}
arrhythmia_hf[, 2] <- h2o.asfactor(arrhythmia_hf[, 2])
arrhythmia_hf[, 3] <- h2o.asfactor(arrhythmia_hf[, 3])

splits <- h2o.splitFrame(arrhythmia_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

response <- "diagnosis"
weights <- "weights"
features <- setdiff(colnames(train), c(response, weights, "class"))
```

<br>

### Grid Search

We can use the *h2o.grid()* function to perform e.g. Random Grid Search (RGS). We could also test all possible combinations of parameters with Cartesian Grid Search, but RGS is much faster and usually finds sufficiently accurate models.

For RGS we first define a set of hyper-parameters and search criteria, the grid. We can also specify how long we want to run the grid search for. Based on the results of each model tested in the grid, we can choose the one with the highest accuracy or best performance for the question on hand.

---

What Are Hyperparameters?
Nearly all model algorithms used in machine learning have a set of tuning “knobs” which affect how the learning algorithm fits the model to the data. Examples are the regularization settings alpha and lambda for Generalized Linear Modeling or ntrees and max_depth for Gradient Boosted Models. These knobs are called hyperparameters to distinguish them from internal model parameters, such as GLM’s beta coefficients or Deep Learning’s weights, which get learned from the data during the model training process.

What Is Hyperparameter Optimization?
The set of all combinations of values for these knobs is called the hyperparameter space. We’d like to find a set of hyperparameter values which gives us the best model for our data in a reasonable amount of time. This process is called hyperparameter optimization.

H2O contains good default values for many datasets, but to get the best performance for your data you will want to tune at least some of these hyperparameters to maximize the predictive performance of your models. You should start with the most important hyperparameters for your algorithm of choice, for example ntrees and max_depth for the tree models or the hidden layers for Deep Learning.

H2O provides some guidance by grouping the hyperparameters by their importance in the Flow UI. You should look carefully at the values of the ones marked critical, while the secondary or expert ones are generally used for special cases or fine tuning.

Note that some hyperparameters, such as learning_rate, have a very wide dynamic range. You should choose values that reflect this for your search (e.g., powers of 10 or of 2) to ensure that you cover the most relevant parts of the hyperparameter space. (Bergstra and Bengio p. 290)

Measuring Model Quality

There are many different ways to measure model quality. If you don’t know which to use, H2O will choose a good general-purpose metric for you based on the category of your model (binomial or multinomial classification, regression, clustering, …). However, you may want to choose a metric to compare your models based on your specific goals (e.g., maximizing AUC, minimizing log loss, minimizing false negatives, minimizing mean squared error, …).

Overfitting

Overfitting is the phenomenon of fitting a model so thoroughly to your training data that it begins to memorize the fine details of that specific data, rather than finding general characteristics of that data which will also apply to future data on which you want to make predictions.

Overfitting not only applies to the model training process, but also to the model selection process. During the process of tuning the hyperparameters and selecting the best model you should avoid overfitting them to your training data. Otherwise, the hyperparameter values that you choose will be too highly tuned to your selection data, and will not generalize as well as they could to new data. Note that this is the same principle as, but subtly different from, overfitting during model training. Ideally you should use cross-validation or a validation set during training and then a final holdout test (validation) dataset for model selection. As Bergstra and Bengio write on p. 290,

The standard practice for evaluating a model found by cross-validation is to report [test set error] for the [hyperparameter vector] that minimizes [validation error].

You can read much more on this topic in Chapter 7 of Elements of Statistical Learning from H2O advisors and Stanford professors Trevor Hastie and Rob Tibshirani with Jerome Friedman [2].

Selecting Hyperparameters Manually and With Cartesian Grid
The traditional method of selecting the values for your hyperparameters has been to individually train a number of models with different combinations of values, and then to compare the model performance to choose the best model. For example, for a tree-based model you might choose ntrees of (50, 100 and 200) and max_depth of (5, 10, 15 and 20) for a total of 3 x 4 = 12 models. This process of trying out hyperparameter sets by hand is called manual search. By looking at the models’ predictive performance, as measured by test-set, cross-validation or validation metrics, you select the best hyperparameter settings for your data and needs.

As the number of hyperparameters and the lists of desired values increase this obviously becomes quite tedious and difficult to manage.

A Little Help?

For several years H2O has included grid search, also known as Cartesian Hyperparameter Search or exhaustive search. Grid search builds models for every combination of hyperparameter values that you specify.

Bergstra and Bengio write on p. 283:

Grid search … typically finds a better [set of hyperparameters] than purely manual sequential optimization (in the same amount of time)

H2O keeps track of all the models resulting from the search, and allows you to sort the list based on any supported model metric (e.g., AUC or log loss). For the example above, H2O would build your 12 models and return the list sorted with the best first, either using the metric of your choice or automatically using one that’s generally appropriate for your model’s category.

H2O allows you to run multiple hyperparameter searches and to collect all the models for comparison in a single sortable result set: just name your grid and run multiple searches. You can even add models from manual searches to the result set by specifying a grid search with a single value for each interesting hyperparameter:

---
#### Activation Functions
- **Rectifier**: is the default activation function. It is the fastest and most versatile option. It can lead to instabilities though and tends to be lower in accuracy.
- **Tanh**: ia a scaled and shifted variant of the sigmoid activation function. It can take on values from -1 to 1 and centers around 0. Tanh needs more computational power than e.g. the Rectifier function.
- **Maxout**: is computationally quite demanding but can produce high accuracy models.

- **...WithDropout**: works together with the parameter *hidden_dropout_ratios* (ontrols the amount of layer neurons that are randomly dropped for each hidden layer). Hidden dropout ratios are useful for preventing overfitting on learned features.

#### L1 and L2 penalties
- **L1**: lets only strong weights survive
- **L2**: prevents any single weight from getting too big. 

```{r}
hyper_params <- list(
                     activation = c("Rectifier", "Maxout", "Tanh"), #, "RectifierWithDropout", "MaxoutWithDropout", "TanhWithDropout"), 
                     hidden = list(c(10, 10, 10), c(50, 50, 50), c(100, 100, 100))
                     #rate = c(0.01, 0.02),
                     #l1 = c(0, 0.00001, 0.0001), 
                     #l2 = c(0, 0.00001, 0.0001)
                     #rate_annealing = c(1e-8, 1e-7, 1e-6)
                     )
```

#### Early stopping criteria
- **stopping_metric**: metric that we want to use as stopping criterion
- **stopping_tolerance** and **stopping_rounds**: trainig stops when the the stopping metric does not improve by the stopping tolerance proportion any more (e.g. by 0.05 or 5%) for the number of consecutive rounds defined by stopping rounds.

```{r}
search_criteria <- list(strategy = "RandomDiscrete", 
                        max_models = 100,
                        max_runtime_secs = 360,
                        stopping_tolerance = 0.001,
                        stopping_rounds = 10,
                        seed = 42)
```

```{r}
dl_grid <- h2o.grid(algorithm = "deeplearning", 
                    x = features,
                    y = response,
                    weights_column = weights,
                    grid_id = "dl_grid",
                    training_frame = train,
                    validation_frame = valid,
                    epochs = 100,
                    nfolds = 15,                                   
                    keep_cross_validation_fold_assignment = TRUE,
                    fold_assignment = "Stratified",
                    score_each_iteration = TRUE,
                    variable_importances = TRUE,
                    export_weights_and_biases = TRUE,                   
                    hyper_params=hyper_params,
                    search_criteria = search_criteria
                    )
```

```{r}
grid <- h2o.getGrid("dl_grid",sort_by="err",decreasing=FALSE)

## To see what other "sort_by" criteria are allowed
#grid <- h2o.getGrid("dl_grid",sort_by="wrong_thing",decreasing=FALSE)

```

```{r}
model_ids <- grid@model_ids
best_model <- h2o.getModel(model_ids[[1]])
```

```{r }
h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)
```

Now let’s evaluate the model performance on a test set so we get an honest estimate of top model performance.

```{r}
perf <- h2o.performance(best_model, test)
plot(perf)
h2o.mse(perf)
h2o.auc(perf)
h2o.confusionMatrix(best_model, test)
```



####Training Samples per MapReduce Iteration
The parameter `train_samples_per_iteration` matters especially in multi-node operation. It controls the number of rows trained on for each MapReduce iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of `-2` indicates auto-tuning, which attemps to keep the communication overhead at 5% of the total runtime. The parameter `target_ratio_comm_to_comp` controls this ratio. This parameter is explained in more detail in the [H2O Deep Learning booklet](http://h2o.ai/resources/),


####Loss functions, Distributions, Offsets, Observation Weights
H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights.
In addition to `Gaussian` distributions and `Squared` loss, H2O Deep Learning supports `Poisson`, `Gamma`, `Tweedie` and `Laplace` distributions. It also supports `Absolute` and `Huber` loss and per-row offsets specified via an `offset_column`. Observation weights are supported via a user-specified `weights_column`.


`rho` and `epsilon`, which balance the global and local search efficiencies. `rho` is the similarity to prior weight updates (similar to momentum), and `epsilon` is a parameter that prevents the optimization to get stuck in local optima. Defaults are `rho=0.99` and `epsilon=1e-8`. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with `rho in c(0.9,0.95,0.99,0.999)` and `epsilon in c(1e-10,1e-8,1e-6,1e-4)`). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).


In the context of artificial neural networks, the rectifier is an activation function defined as

{\displaystyle f(x)=\max(0,x),} {\displaystyle f(x)=\max(0,x),}
where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature[1] with strong biological motivations and mathematical justifications.[2] It has been used in convolutional networks[3] more effectively than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical[4] counterpart, the hyperbolic tangent. The rectifier is, as of 2015, the most popular activation function for deep neural networks.[5]

A unit employing the rectifier is also called a rectified linear unit (ReLU).[6]

A smooth approximation to the rectifier is the analytic function

{\displaystyle f(x)=\ln(1+e^{x}),} {\displaystyle f(x)=\ln(1+e^{x}),}
which is called the softplus function.[7] The derivative of softplus is {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})} {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})}, i.e. the logistic function.

Rectified linear units find applications in computer vision[3] and speech recognition[8][9] using deep neural nets.

A maxout layer is simply a layer where the activation function is the max of the inputs. As stated in the paper, even an MLP with 2 maxout units can approximate any function. They give a couple of reasons as to why maxout may be performing well, but the main reason they give is the following --

Dropout can be thought of as a form of model averaging in which a random subnetwork is trained at every iteration and in the end the weights of the different such random networks are averaged. Since one cannot average the weights explicitly, an approximation is used. This approximation is exact for for a linear network 
In maxout, they do not drop the inputs to the maxout layer. Thus the identity of the input outputting the max value for a data point remains unchanged. Thus the dropout only happens in the linear part of the MLP but one can still approximate any function because of the maxout layer. 
As the dropout happens in the linear part only, they conjecture that this leads to more efficient model averaging as the averaging approximation is exact for linear networks.

tanh: Hyperbolic tangent:
Although tanh is just a scaled and shifted version of a logistic sigmoid, one of the prime reasons why tanh is the preferred activation/transfer function is because it squashes to a wider numerical range (-1..1) and has asymptotic symmetry.

There are several implications to these properties,

It results in faster convergence than the standard logistic function.
On average, it's more likely to create output values that are close to 0, which is beneficial when forward propagating to subsequent layers.
Overall makes training less difficult as it less proned to saturation in the later layers of your network.
When supplying transformed or normalized inputs, the variance of the outputs will also be close to 1. This makes it great for when you're training on image corpora.
It also has a nice derivative (1-tanh^2)! For training purposes, you can calculate the value of the derivative just by using the output, no knowledge of the input is needed.
Yann Lecun's shares some great insight in his paper Efficient Backpropagation on why tanh works well in-practice.

From my own experience, I've found tanh particularly beneficial when used with deep-convolutional neural networks. The intuition behind choosing an ideal activation functions moreso depending on the nature of your problem, dataset, and type of network.

It's best to test it out yourself and compare the results emprically; there are many times where it won't make much of a difference at all (especially in shallow networks).

On the field of Artificial Neural Networks, the sigmoid funcion is a type of activation function for artifical neurons.

The most basic activation funciton is the Heaviside (binary step, 0 or 1, high or low):

You can have several types of activation functions and they are best suitable for different purposes. In the specific case of the Sigmoid:

Real-valued and differentiable (you need this to find gradients);
Analytic tractability for the differentiaton operation;
It is an acceptable mathematical representation of a biological neuron behaviour. The output shows if the neuron is firing or not.
There is a table with the various types of activation functions:

Activation function

Most of the time you will be concerned about the following points when choosing the activation function:

continuity of the function
computational power to process all neurons of the network
type of the desired output (logistic/continuous variables or classification/categorical data)




Let's see which model had the lowest validation error:

```{r}
scores <- cbind(as.data.frame(unlist((lapply(dl_grid@model_ids, function(x)
{ h2o.confusionMatrix(h2o.performance(h2o.getModel(x),valid=T))$Error[3] })) )), unlist(dl_grid@model_ids))

names(scores) <- c("misclassification","model")
sorted_scores <- scores[order(scores$misclassification),]
head(sorted_scores)
best_model <- h2o.getModel(as.character(sorted_scores$model[1]))
print(best_model@allparameters)
best_err <- sorted_scores$misclassification[1]
print(best_err)
```

### Random Hyper-Parameter Search

Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build `N` models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters.

```{r}
models <- c()

for (i in 1:10) {

  rand_activation <- c("TanhWithDropout", "RectifierWithDropout")[sample(1:2, 1)]
  rand_numlayers <- sample(2:5,1)
  rand_hidden <- c(sample(10:50, rand_numlayers, T))
  rand_l1 <- runif(1, 0, 1e-3)
  rand_l2 <- runif(1, 0, 1e-3)
  rand_dropout <- c(runif(rand_numlayers, 0, 0.6))
  rand_input_dropout <- runif(1, 0, 0.5)
  
  dlmodel <- h2o.deeplearning(model_id = paste0("dl_random_model_", i),
                              training_frame = train,
                              validation_frame = valid,
                              x = features,
                              y = response,
                              nfolds = 10,
                              max_w2 = 10,                      ## can help improve stability for Rectifier
                              
                              ### Random parameters
                              activation = rand_activation,
                              hidden = rand_hidden,
                              l1 = rand_l1,
                              l2 = rand_l2,
                              input_dropout_ratio = rand_input_dropout,
                              hidden_dropout_ratios = rand_dropout)
  
  models <- c(models, dlmodel)
}
```

We continue to look for the model with the lowest validation misclassification rate:

```{r}
best_err <- 1
for (i in 1:length(models)) {
  err <- h2o.confusionMatrix(h2o.performance(models[[i]],valid=T))$Error[3]
  if (err < best_err) {
    best_err <- err
    best_model <- models[[i]]
  }
}
h2o.confusionMatrix(best_model,valid=T)
best_params <- best_model@allparameters
best_params$hidden
best_params$l1
best_params$l2
best_params$input_dropout_ratio
```

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as `epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance` can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

```{r}
max_epochs <- 12 ## Add two more epochs
m_cont <- h2o.deeplearning(model_id="dl_model_tuned_continued",
                          training_frame=train,
                          validation_frame=valid,
                          x=features,
                          y=response,
                          hidden=c(128,128,128),          ## more hidden layers -> more complex interactions
                          epochs=max_epochs,              ## hopefully long enough to converge (otherwise restart again)
                          stopping_metric="logloss",      ## logloss is directly optimized by Deep Learning
                          stopping_tolerance=1e-2,        ## stop when validation logloss does not improve by >=1% for 2 scoring events
                          stopping_rounds=2,
                          score_validation_samples=10000, ## downsample validation set for faster scoring
                          score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
                          adaptive_rate=F,                ## manually tuned learning rate
                          rate=0.01,
                          rate_annealing=2e-6,
                          momentum_start=0.2,             ## manually tuned momentum
                          momentum_stable=0.4,
                          momentum_ramp=1e7,
                          l1=1e-5,                        ## add some L1/L2 regularization
                          l2=1e-5,
                          max_w2=10                       ## helps stability for Rectifier
)
summary(m_cont)
plot(m_cont)
```

