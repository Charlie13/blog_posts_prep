---
title: "Grid Search"
author: "Shirin Glander"
date: "February 21, 2017"
output: html_document
---

### Hyper-parameter Tuning with Grid Search

```{r}
arrhythmia <- read.table("arrhythmia.data.txt", sep = ",")

arrhythmia[-280] <- lapply(arrhythmia[-280], as.numeric)

colnames(arrhythmia)[280] <- "class"
arrhythmia$class <- as.factor(arrhythmia$class)
```

```{r}
arrhythmia$class <- ifelse(arrhythmia$class == 1, "normal", "arrhythmia")

library(matrixStats)

colvars <- data.frame(feature = colnames(arrhythmia[-280]),
                      variance = colVars(as.matrix(arrhythmia[-280])))

arrhythmia_subset <- cbind(arrhythmia[, c(280, which(colvars$variance > 50))])
```

```{r}
arrhythmia_sc <- copy_to(sc, arrhythmia_subset, overwrite = TRUE)
arrhythmia_hf <- as_h2o_frame(sc, arrhythmia_sc, strict_version_check = FALSE)
```

```{r}
arrhythmia_hf[, 1] <- h2o.asfactor(arrhythmia_hf[, 1])

splits <- h2o.splitFrame(arrhythmia_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

response <- "class"
features <- setdiff(colnames(train), c(response, weights))
```

Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning:

As an alternative to manual tuning, or “hand tuning”, we can use the h2o.grid() function to perform either a Cartesian or Random Grid Search (RGS). Random Grid Search is usually a quicker way to find a good model. The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search:

One handy feature of RGS is that you can specify how long you would like to execute the grid for – this can be based on a time, number of models, or a performance-metric-based stopping criterion.

First define a grid of Deep Learning hyperparameters and specify the search_criteria. Once we have trained the grid, we can collect the results and sort by our model performance metric of choice.

In regression analysis, the term mean squared error is sometimes used to refer to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom. This definition for a known, computed quantity differs from the above definition for the computed MSE of a predictor in that a different denominator is used. The denominator is the sample size reduced by the number of model parameters estimated from the same data, (n-p) for p regressors or (n-p-1) if an intercept is used.[3] For more details, see errors and residuals in statistics. Note that, although the MSE (as defined in the present article) is not an unbiased estimator of the error variance, it is consistent, given the consistency of the predictor.

Also in regression analysis, "mean squared error", often referred to as mean squared prediction error or "out-of-sample mean squared error", can refer to the mean value of the squared deviations of the predictions from the true values, over an out-of-sample test space, generated by a model estimated over a particular sample space. This also is a known, computed quantity, and it varies by sample and by out-of-sample test space.

```{r}
activation_opt <- c("Rectifier", "Maxout", "Tanh", "Sigmoid")
l1_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01)
l2_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01)

hyper_params <- list(activation = activation_opt, 
                     l1 = l1_opt, 
                     l2 = l2_opt)

search_criteria <- list(strategy = "RandomDiscrete", max_runtime_secs = 60) # train grid for 60 seconds
```

In the context of artificial neural networks, the rectifier is an activation function defined as

{\displaystyle f(x)=\max(0,x),} {\displaystyle f(x)=\max(0,x),}
where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature[1] with strong biological motivations and mathematical justifications.[2] It has been used in convolutional networks[3] more effectively than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical[4] counterpart, the hyperbolic tangent. The rectifier is, as of 2015, the most popular activation function for deep neural networks.[5]

A unit employing the rectifier is also called a rectified linear unit (ReLU).[6]

A smooth approximation to the rectifier is the analytic function

{\displaystyle f(x)=\ln(1+e^{x}),} {\displaystyle f(x)=\ln(1+e^{x}),}
which is called the softplus function.[7] The derivative of softplus is {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})} {\displaystyle f'(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})}, i.e. the logistic function.

Rectified linear units find applications in computer vision[3] and speech recognition[8][9] using deep neural nets.

A maxout layer is simply a layer where the activation function is the max of the inputs. As stated in the paper, even an MLP with 2 maxout units can approximate any function. They give a couple of reasons as to why maxout may be performing well, but the main reason they give is the following --

Dropout can be thought of as a form of model averaging in which a random subnetwork is trained at every iteration and in the end the weights of the different such random networks are averaged. Since one cannot average the weights explicitly, an approximation is used. This approximation is exact for for a linear network 
In maxout, they do not drop the inputs to the maxout layer. Thus the identity of the input outputting the max value for a data point remains unchanged. Thus the dropout only happens in the linear part of the MLP but one can still approximate any function because of the maxout layer. 
As the dropout happens in the linear part only, they conjecture that this leads to more efficient model averaging as the averaging approximation is exact for linear networks.

tanh: Hyperbolic tangent:
Although tanh is just a scaled and shifted version of a logistic sigmoid, one of the prime reasons why tanh is the preferred activation/transfer function is because it squashes to a wider numerical range (-1..1) and has asymptotic symmetry.

There are several implications to these properties,

It results in faster convergence than the standard logistic function.
On average, it's more likely to create output values that are close to 0, which is beneficial when forward propagating to subsequent layers.
Overall makes training less difficult as it less proned to saturation in the later layers of your network.
When supplying transformed or normalized inputs, the variance of the outputs will also be close to 1. This makes it great for when you're training on image corpora.
It also has a nice derivative (1-tanh^2)! For training purposes, you can calculate the value of the derivative just by using the output, no knowledge of the input is needed.
Yann Lecun's shares some great insight in his paper Efficient Backpropagation on why tanh works well in-practice.

From my own experience, I've found tanh particularly beneficial when used with deep-convolutional neural networks. The intuition behind choosing an ideal activation functions moreso depending on the nature of your problem, dataset, and type of network.

It's best to test it out yourself and compare the results emprically; there are many times where it won't make much of a difference at all (especially in shallow networks).

On the field of Artificial Neural Networks, the sigmoid funcion is a type of activation function for artifical neurons.

The most basic activation funciton is the Heaviside (binary step, 0 or 1, high or low):

You can have several types of activation functions and they are best suitable for different purposes. In the specific case of the Sigmoid:

Real-valued and differentiable (you need this to find gradients);
Analytic tractability for the differentiaton operation;
It is an acceptable mathematical representation of a biological neuron behaviour. The output shows if the neuron is firing or not.
There is a table with the various types of activation functions:

Activation function

Most of the time you will be concerned about the following points when choosing the activation function:

continuity of the function
computational power to process all neurons of the network
type of the desired output (logistic/continuous variables or classification/categorical data)

```{r}
dl_grid <- h2o.grid("deeplearning", 
                    x = features,
                    y = response,
                    grid_id = "dl_grid",
                    training_frame = train,
                    validation_frame = valid,
                    seed = 1234,
                    hidden = c(200, 200, 200, 200),
                    nfolds = 10,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria)
```

```{r}
dl_gridperf <- h2o.getGrid(grid_id = "dl_grid",
                           sort_by = "accuracy",
                           decreasing = TRUE)

print(dl_gridperf)
```

Grab the model_id for the top DL model, chosen by validation error.

```{r}
best_dl_model_id <- dl_gridperf@model_ids[[1]]
best_dl <- h2o.getModel(best_dl_model_id)
```

Now let’s evaluate the model performance on a test set so we get an honest estimate of top model performance.

```{r}
best_dl_perf <- h2o.performance(model = best_dl, newdata = test)
h2o.mse(best_dl_perf)
```

Let's see which model had the lowest validation error:

```{r}
scores <- cbind(as.data.frame(unlist((lapply(dl_grid@model_ids, function(x)
{ h2o.confusionMatrix(h2o.performance(h2o.getModel(x),valid=T))$Error[3] })) )), unlist(dl_grid@model_ids))

names(scores) <- c("misclassification","model")
sorted_scores <- scores[order(scores$misclassification),]
head(sorted_scores)
best_model <- h2o.getModel(as.character(sorted_scores$model[1]))
print(best_model@allparameters)
best_err <- sorted_scores$misclassification[1]
print(best_err)
```

### Random Hyper-Parameter Search

Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build `N` models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters.

```{r}
models <- c()

for (i in 1:10) {

  rand_activation <- c("TanhWithDropout", "RectifierWithDropout")[sample(1:2, 1)]
  rand_numlayers <- sample(2:5,1)
  rand_hidden <- c(sample(10:50, rand_numlayers, T))
  rand_l1 <- runif(1, 0, 1e-3)
  rand_l2 <- runif(1, 0, 1e-3)
  rand_dropout <- c(runif(rand_numlayers, 0, 0.6))
  rand_input_dropout <- runif(1, 0, 0.5)
  
  dlmodel <- h2o.deeplearning(model_id = paste0("dl_random_model_", i),
                              training_frame = train,
                              validation_frame = valid,
                              x = features,
                              y = response,
                              nfolds = 10,
                              max_w2 = 10,                      ## can help improve stability for Rectifier
                              
                              ### Random parameters
                              activation = rand_activation,
                              hidden = rand_hidden,
                              l1  =rand_l1,
                              l2 = rand_l2,
                              input_dropout_ratio = rand_input_dropout,
                              hidden_dropout_ratios = rand_dropout)
  
  models <- c(models, dlmodel)
}
```

We continue to look for the model with the lowest validation misclassification rate:

```{r}
best_err <- 1
for (i in 1:length(models)) {
  err <- h2o.confusionMatrix(h2o.performance(models[[i]],valid=T))$Error[3]
  if (err < best_err) {
    best_err <- err
    best_model <- models[[i]]
  }
}
h2o.confusionMatrix(best_model,valid=T)
best_params <- best_model@allparameters
best_params$hidden
best_params$l1
best_params$l2
best_params$input_dropout_ratio
```

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as `epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance` can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

```{r}
max_epochs <- 12 ## Add two more epochs
m_cont <- h2o.deeplearning(model_id="dl_model_tuned_continued",
                          training_frame=train,
                          validation_frame=valid,
                          x=features,
                          y=response,
                          hidden=c(128,128,128),          ## more hidden layers -> more complex interactions
                          epochs=max_epochs,              ## hopefully long enough to converge (otherwise restart again)
                          stopping_metric="logloss",      ## logloss is directly optimized by Deep Learning
                          stopping_tolerance=1e-2,        ## stop when validation logloss does not improve by >=1% for 2 scoring events
                          stopping_rounds=2,
                          score_validation_samples=10000, ## downsample validation set for faster scoring
                          score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
                          adaptive_rate=F,                ## manually tuned learning rate
                          rate=0.01,
                          rate_annealing=2e-6,
                          momentum_start=0.2,             ## manually tuned momentum
                          momentum_stable=0.4,
                          momentum_ramp=1e7,
                          l1=1e-5,                        ## add some L1/L2 regularization
                          l2=1e-5,
                          max_w2=10                       ## helps stability for Rectifier
)
summary(m_cont)
plot(m_cont)
```

```{r}
# Grid Search

# GBM hyperparamters
gbm_params1 <- list(learn_rate = c(0.01, 0.1),
                    max_depth = c(3, 5, 9),
                    sample_rate = c(0.8, 1.0),
                    col_sample_rate = c(0.2, 0.5, 1.0))

# Train and validate a grid of GBMs
gbm_grid1 <- h2o.grid("gbm", 
                      x = features, 
                      y = response,
                      grid_id = "gbm_grid1",
                      training_frame = splits[[1]],
                      validation_frame = splits[[2]],
                      ntrees = 100,
                      seed = 1,
                      hyper_params = gbm_params1)

# Get the grid results, sorted by validation MSE
gbm_gridperf1 <- h2o.getGrid(grid_id = "gbm_grid1", 
                             sort_by = "mse", 
                             decreasing = FALSE)

print(gbm_gridperf1)
```


This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. Note that binary compatibility between H2O versions is currently not guaranteed.
