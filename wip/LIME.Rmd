---
title: "LIME"
author: "Dr. Shirin Glander"
date: "April 21, 2017"
output: html_document
---

https://github.com/thomasp85/lime

https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime



```{r}
load("oneR/data_15_16.RData")
```

```{r warning=FALSE, message=FALSE}
# configure multicore
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)
```

```{r}
set.seed(42)
index <- createDataPartition(data_15_16$Happiness.Score.l, p = 0.7, list = FALSE)
train_data <- data_15_16[index, ]
test_data  <- data_15_16[-index, ]
```

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=4, eval=FALSE}
set.seed(42)
model_rf <- caret::train(Happiness.Score.l ~ .,
                         data = train_data,
                         method = "rf",
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 5, 
                                                  verboseIter = FALSE))
```

```{r echo=FALSE}
load("oneR/model_rf.RData")
```

```{r warning=FALSE, message=FALSE}
pred <- data.frame(sample_id = 1:nrow(test_data),
                   predict(model_rf, test_data, type = "prob"),
                   actual = test_data$Happiness.Score.l)
  pred$prediction <- colnames(pred)[3:5][apply(pred[, 3:5], 1, which.max)]
  pred$correct <- ifelse(pred$actual == pred$prediction, "correct", "wrong")
```

```{r eval=FALSE}
devtools::install_github("thomasp85/lime")
```

The central function of **lime** is the `lime()` function.

feature_select: The method to use for feature selection. One of:

"auto": If n_features <= 6 use "forward_selection" else use "highest_weights".

"none": Ignore n_features and use all features.

"forward_selection": Add one feature at a time until n_features is reached, based on quality of a ridge regression model.

"highest_weights": Fit a ridge regression and select the n_features with the highest absolute weight.

"lasso_path": Fit a lasso model and choose the n_features whose lars path converge to zero the latest.

Here, we only have 7 features, I want to look at the top 5.

```{r}
library(lime)

# Create explanation function
explain <- lime(train_data, model_rf, bin_continuous = TRUE, n_bins = 5, n_labels = 3, n_features = 5, n_permutations = 1000)
```

Now, let's look at how the model is explained. I am not going to look at all test cases but I'm randomly choosing three cases with correct predictions and three with wrong predictions.

```{r}
pred_cor <- filter(pred, correct == "correct")
pred_wrong <- filter(pred, correct == "wrong")

test_data_cor <- test_data %>%
  mutate(sample_id = 1:nrow(test_data)) %>%
  filter(sample_id %in% pred_cor$sample_id) %>%
  sample_n(size = 3) %>%
  select(-sample_id, - Happiness.Score.l)

test_data_wrong <- test_data %>%
  mutate(sample_id = 1:nrow(test_data)) %>%
  filter(sample_id %in% pred_wrong$sample_id) %>%
  sample_n(size = 3) %>%
  select(-sample_id, - Happiness.Score.l)
```

```{r}
# Explain new observation
explanation_cor <- explain(test_data_cor, n_labels = 3, n_features = 5)
explanation_wrong <- explain(test_data_wrong, n_labels = 3, n_features = 5)
```

```{r warning=FALSE, message=FALSE, fig.height=10, fig.width=20}
plot_features(explanation_cor, ncol = 3)
```

```{r warning=FALSE, message=FALSE, fig.height=10, fig.width=20}
plot_features(explanation_wrong, ncol = 3)
```

The return value of the returned function will be a tibble encoding the explanations in a tidy format. The columns are:

case: The case being explained (the rowname in cases)

predict_label: The label with the highest probability as predicted by model

predict_prob: The probability of predict_label

label: The label being explained

label_prob: The probability of label as predicted by model

feature: The feature used for the explanation

weight: The weight of the feature in the explanation

model_r2: The quality of the model used for the explanation

model_intercept: The intercept of the model used for the explanation

```{r}
# The output is provided in a nice tidy format
tibble::glimpse(explanation_cor)
```


