---
title: "Feature Selection in Machine Learning (Predicting Breast Cancer)"
author: "Shirin Glander"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

Machine Learning uses so called features (i.e. variables or attributes) to generate predictive models. Because using too many (unspecific) features for model training poses the problem of overfitting, we generally want to restrict the features in our models to the most repesentative for the response variable. Using a suitable combination of features is essential for obtaining high precision and accuracy of our model.

There are several ways to identify how much each feature contributes to the model and to restrict the number of selected features. Here, I am going to examine:

- Correlation
- Recursive Feature Elimination
- Genetic Algorithm


## Breast Cancer Wisconsin (Diagnostic) Data Set

The data I am going to use to explore feature selection methods is the Breast Cancer Wisconsin (Diagnostic) Data Set:

> W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. 

> O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. 

> W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. 

> W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Image analysis and machine learning applied to breast cancer diagnosis and prognosis. Analytical and Quantitative Cytology and Histology, Vol. 17 No. 2, pages 77-87, April 1995. 

> W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. Computerized breast cancer diagnosis and prognosis from fine needle aspirates. Archives of Surgery 1995;130:511-516.

> W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. Computer-derived nuclear features distinguish malignant from benign breast cytology. Human Pathology, 26:792--796, 1995.

Included are three datasets. The features in these datasets characterise cell nucleus properties and were generated from image analysis of [fine needle aspirates (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration) of breast masses. The data was downloaded from the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).

The first datasets encompasses only 9 features, the other two datasets have 30 and 33 features. Here, I want to explore the effect feature selection on model prediction.


### Breast cancer dataset 1

The first dataset looks at the predictor classes:

- M: malignant or
- B: benign breast mass.

The phenotypes for characterisation are:

- Sample ID (code number)
- Clump thickness
- Uniformity of cell size
- Uniformity of cell shape
- Marginal adhesion
- Single epithelial cell size
- Number of bare nuclei
- Bland chromatin
- Number of normal nuclei
- Mitosis
- Classes, i.e. diagnosis

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 6, fig.height = 5, fig.align = "center"}
bc_data <- read.table("breast-cancer-wisconsin.data.txt", header = FALSE, sep = ",")
colnames(bc_data) <- c("sample_code_number", "clump_thickness", "uniformity_of_cell_size", "uniformity_of_cell_shape", "marginal_adhesion", "single_epithelial_cell_size", 
                       "bare_nuclei", "bland_chromatin", "normal_nucleoli", "mitosis", "classes")
bc_data$classes <- ifelse(bc_data$classes == "2", "benign",
                          ifelse(bc_data$classes == "4", "malignant", NA))

bc_data[bc_data == "?"] <- NA

# how many NAs are in the data
length(which(is.na(bc_data)))

# impute missing data
library(mice)

bc_data[,2:10] <- apply(bc_data[,2:10], 2, function(x) as.numeric(as.character(x)))
dataset_impute <- mice(bc_data[,2:10],  print = FALSE)
bc_data <- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, 1))

str(bc_data)
```

### Breast cancer dataset 2

The second dataset looks again at the predictor classes:

- M: malignant or
- B: benign breast mass.

The first two columns give:

- Sample ID 
- Classes, i.e. diagnosis

For each cell nucleus, the following ten characteristics were measured:

- Radius (mean of all distances from the center to points on the perimeter)
- Texture (standard deviation of gray-scale values)
- Perimeter
- Area
- Smoothness (local variation in radius lengths)
- Compactness (perimeter^2 / area - 1.0)
- Concavity (severity of concave portions of the contour)
- Concave points (number of concave portions of the contour)
- Symmetry 
- Fractal dimension ("coastline approximation" - 1)

For each characteristic three measures are given:

- Mean
- Standard error
- Largest/ "worst"

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 6, fig.height = 5, fig.align = "center"}
bc_data_2 <- read.table("wdbc.data.txt", header = FALSE, sep = ",")

phenotypes <- rep(c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_points", "symmetry", "fractal_dimension"), 3)
types <- rep(c("mean", "se", "largest_worst"), each = 10)

colnames(bc_data_2) <- c("ID", "diagnosis", paste(phenotypes, types, sep = "_"))

# how many NAs are in the data
length(which(is.na(bc_data_2)))

str(bc_data_2)
```

### Breast cancer dataset 3

The third dataset looks at the predictor classes:

- R: recurring or
- N: nonrecurring breast cancer.

The first two columns give:

- Sample ID 
- Classes, i.e. outcome

For each cell nucleus, the same ten characteristics and measures were given as in dataset 2, plus:

- Time (recurrence time if field 2 = R, disease-free time if field 2	= N)
- Tumor size - diameter of the excised tumor in centimeters
- Lymph node status - number of positive axillary lymph nodes observed at time of surgery

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 6, fig.height = 5, fig.align = "center"}
bc_data_3 <- read.table("wpbc.data.txt", header = FALSE, sep = ",")
colnames(bc_data_3) <- c("ID", "outcome", "time", paste(phenotypes, types, sep = "_"), "tumor_size", "lymph_node_status")

bc_data_3[bc_data_3 == "?"] <- NA

# how many NAs are in the data
length(which(is.na(bc_data_3)))

# impute missing data
library(mice)

bc_data_3[,3:35] <- apply(bc_data_3[,3:35], 2, function(x) as.numeric(as.character(x)))
dataset_impute <- mice(bc_data_3[,3:35],  print = FALSE)
bc_data_3 <- cbind(bc_data_3[, 2, drop = FALSE], mice::complete(dataset_impute, 1))

str(bc_data_3)
```

## Principal Component Analysis (PCA)

To get an idea about the dimensionality and variance of the datasets, I am first looking at PCA plots for samples and features.

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
# preparing plot theme

library(ggplot2)

my_theme <- function(base_size = 12, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "aliceblue"),
    strip.background = element_rect(fill = "lightgrey", color = "grey", size = 1),
    strip.text = element_text(face = "bold", size = 12, color = "black"),
    legend.position = "right",
    legend.justification = "top", 
    legend.background = element_blank(),
    panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )
}

theme_set(my_theme())
```

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
# preparing function for PCA plotting

library(ellipse)

pca_func <- function(data, groups, title, print_ellipse = TRUE) {
  pcaOutput <- pcaGoPromoter::pca(data, printDropped = FALSE, scale=TRUE, center=TRUE)
  pcaOutput2 <- as.data.frame(pcaOutput$scores)
  pcaOutput2$groups <- groups
  
  if (print_ellipse) {
    centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)
    conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
      data.frame(groups = as.character(t),
                 ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                       centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                       level = 0.95),
                 stringsAsFactors = FALSE)))
    
    plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
      geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
      geom_point(size = 2, alpha = 0.6) + 
      scale_colour_brewer(palette = "Set1") +
      labs(title = title,
           color = "",
           fill = "",
           x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2), "% variance"),
           y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2), "% variance"))
  } else {
    
    if (length(unique(pcaOutput2$groups)) <= 10) {
      plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
        geom_point(size = 2, alpha = 0.6) + 
        scale_colour_brewer(palette = "Set1") +
        labs(title = title,
             color = "",
             fill = "",
             x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2), "% variance"),
             y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2), "% variance"))
    } else {
      plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
        geom_point(size = 2, alpha = 0.6) + 
        labs(title = title,
             color = "",
             fill = "",
             x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2), "% variance"),
             y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2), "% variance"))
    }
  }
  
  return(plot)
}

library(gridExtra)
library(grid)
```

```{r message=FALSE, warning=FALSE, fig.width=12, fig.height=4}
p1 <- pca_func(data = t(bc_data[, 2:10]), groups = as.character(bc_data$classes), title = "Breast cancer dataset 1: Samples")
p2 <- pca_func(data = bc_data[, 2:10], groups = as.character(colnames(bc_data[, 2:10])), title = "Breast cancer dataset 1: Features", print_ellipse = FALSE)
grid.arrange(p1, p2, ncol = 2)
```

```{r message=FALSE, warning=FALSE, fig.width=8, fig.height=5}
h_1 <- hclust(dist(t(bc_data[, 2:10]), method = "euclidean"))
plot(h_1)
```

```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=4}
p1 <- pca_func(data = t(bc_data_2[, 3:32]), groups = as.character(bc_data_2$diagnosis), title = "Breast cancer dataset 2: Samples")
p2 <- pca_func(data = bc_data_2[, 3:32], groups = as.character(colnames(bc_data_2[, 3:32])), title = "Breast cancer dataset 2: Features", print_ellipse = FALSE)
grid.arrange(p1, p2, ncol = 2, widths = c(0.4, 0.6))
```

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
h_2 <- hclust(dist(t(bc_data_2[, 3:32]), method = "euclidean"))
plot(h_2)
```

```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=5}
p1 <- pca_func(data = t(bc_data_3[, 2:34]), groups = as.character(bc_data_3$outcome), title = "Breast cancer dataset 3: Samples")
p2 <- pca_func(data = bc_data_3[, 2:34], groups = as.character(colnames(bc_data_3[, 2:34])), title = "Breast cancer dataset 3: Features", print_ellipse = FALSE)
grid.arrange(p1, p2, ncol = 2, widths = c(0.4, 0.6))
```

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
h_3 <- hclust(dist(t(bc_data_3[,2:34]), method = "euclidean"))
plot(h_3)
```

Datasets 1 and 2 show a nice separation of benign and malignant masses, the features will likely be able to predict these classes quite well for most samples. The classes in dataset 3 are mostly overlapping, here I suspect that prediction will not be as accurate.

The features of datasets 2 and 3 don't cluster very distinctly.


## Feature importance

To get an idea about feature importance I am using a Random Forest algorithm with 10 x 10 cross validation.

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
library(caret)
library(corrplot)
library(doParallel) # parallel processing
registerDoParallel()

# prepare training scheme
control <- trainControl(method = "repeatedcv", number = 10, repeats = 10)

feature_imp <- function(model, title) {
  
  # estimate variable importance
  importance <- varImp(model, scale = TRUE)
  
  importance_df_1 <- importance$importance
  importance_df_1$group <- rownames(importance_df_1)
  
  importance_df_2 <- importance_df_1
  importance_df_2$Overall <- 0
  
  importance_df <- rbind(importance_df_1, importance_df_2)
  
  plot <- ggplot() +
    geom_point(data = importance_df_1, aes(x = Overall, y = group, color = group), size = 2) +
    geom_path(data = importance_df, aes(x = Overall, y = group, color = group, group = group), size = 1) +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) +
    labs(
      x = "Importance",
      y = "",
      title = title,
      subtitle = "Scaled feature importance",
      caption = "\nDetermined with Random Forest and
      repeated cross validation (10 repeats, 10 times)"
    )
  return(plot)
}
```

```{r echo = TRUE, eval=FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
# train the model
set.seed(27)
imp_1 <- train(classes ~ ., data = bc_data, method = "rf", preProcess = c("scale", "center"), trControl = control)
```

```{r echo = FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
load("imp_1.RData")
```

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
imp_1
#str(imp_1$modelInfo)
#str(imp_1$finalModel)

p1 <- feature_imp(imp_1, title = "Breast cancer dataset 1")
```

```{r echo = TRUE, eval=FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
set.seed(27)
imp_2 <- train(diagnosis ~ ., data = bc_data_2[, -1], method = "rf", preProcess = c("scale", "center"), trControl = control)
```

```{r echo = FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
load("imp_2.RData")
```

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
imp_2

p2 <- feature_imp(imp_2, title = "Breast cancer dataset 2")
```

```{r echo = TRUE, eval=FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
set.seed(27)
imp_3 <- train(outcome ~ ., data = bc_data_3, method = "rf", preProcess = c("scale", "center"), trControl = control)
```

```{r echo = FALSE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
load("imp_3.RData")
```

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 5, fig.height = 3, fig.align = "center"}
imp_3

p3 <- feature_imp(imp_3, title = "Breast cancer dataset 3")
```

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 15, fig.height = 8, fig.align = "center"}
grid.arrange(p1, p2, p3, ncol = 3, widths = c(0.3, 0.35, 0.35))
```

## Correlation

Often we have features that are highly correlated with each other and thus provide redundant information. By eliminating highly correlated features we can avoid a predictive bias for the information contained in these features. When we want to make statements about the importance of specific features, we need to keep in mind that just because they are suitable to predicting an outcome they are not necessarily causal for the outcome - they could simply be highly correlated with causal factors.

- Dataset 1

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 6, fig.height = 6, fig.align = "center"}
# calculate correlation matrix
corMatMy <- cor(bc_data[,2:10])
corrplot(corMatMy, order = "hclust")

#Apply correlation filter at 0.70,
highlyCor <- colnames(bc_data[,2:10])[findCorrelation(corMatMy, cutoff=0.7, verbose = TRUE)]

#then we remove these variables
bc_data_cor <- bc_data[, which(!colnames(bc_data) %in% highlyCor)]
```


- Dataset 2

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 10, fig.height = 10, fig.align = "center"}
corMatMy <- cor(bc_data_2[,3:32])
corrplot(corMatMy, order = "hclust")

highlyCor <- colnames(bc_data_2[,3:32])[findCorrelation(corMatMy, cutoff=0.7, verbose = TRUE)]
bc_data_2_cor <- bc_data_2[, which(!colnames(bc_data_2) %in% highlyCor)]
```


- Dataset 3

```{r echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE, fig.width = 10, fig.height = 10, fig.align = "center"}
corMatMy <- cor(bc_data_3[,2:34])
corrplot(corMatMy, order = "hclust")

highlyCor <- colnames(bc_data_3[,2:34])[findCorrelation(corMatMy, cutoff=0.7, verbose = TRUE)]
bc_data_3_cor <- bc_data_3[, which(!colnames(bc_data_3) %in% highlyCor)]
```


## Recursive Feature Elimination (RFE)

RFE uses a Random Forest algorithm to test combinations of features and rates each with an accuracy score. The combination with the highest score is usually preferential.


- Dataset 1

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
# ensure the results are repeatable
set.seed(7)
# define the control using a random forest selection function with cross validation
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

# run the RFE algorithm
results_1 <- rfe(x = bc_data[,-1], y = as.factor(bc_data$classes), sizes = c(1:9), rfeControl = control)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("results_1.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
# summarize the results
print(results_1)
# subset the chosen features
bc_data_rfe <- bc_data[, c(1, which(colnames(bc_data) %in% predictors(results_1)))]
```


- Dataset 2

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(7)
results_2 <- rfe(x = bc_data_2[,-c(1, 2)], y = as.factor(bc_data_2$diagnosis), sizes = c(1:30), rfeControl = control)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("results_2.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
print(results_2)
bc_data_2_rfe <- bc_data_2[, c(2, which(colnames(bc_data_2) %in% predictors(results_2)))]
```


- Dataset 3

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(7)
results_3 <- rfe(x = bc_data_3[,-1], y = as.factor(bc_data_3$outcome), sizes = c(1:33), rfeControl = control)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("results_3.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
print(results_3)
bc_data_3_rfe <- bc_data_3[, c(1, which(colnames(bc_data_3) %in% predictors(results_3)))]
```


## Genetic Algorithm (GA)

For demonstration purposes I am using only 10 generations consisting of 5 individuals. More iterations with larger populations would of course be preferable, but this takes quite long to run!

```{r echo = TRUE, cache = FALSE, message=FALSE}
library(dplyr)
library(pROC) # plot the ROC curve

registerDoParallel(8) # Registrer a parallel backend 

ga_ctrl <- gafsControl(functions = rfGA, # Assess fitness with RF
                       method = "cv",    # 10 fold cross validation
                       genParallel = TRUE, # Use parallel programming
                       allowParallel = TRUE)
```


- Dataset 1

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
lev <- c("malignant", "benign")     # Set the levels

set.seed(27)
model_1 <- gafs(x = bc_data[, -1], y = as.factor(bc_data$classes),
                   iters = 10, # generations of algorithm
                   popSize = 5, # population size for each generation
                   levels = lev,
                   gafsControl = ga_ctrl)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_1.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
model_1

plot(model_1) # Plot mean fitness (AUC) by generation

bc_data_ga <- bc_data[, c(1, which(colnames(bc_data) %in% model_1$ga$final))]
```


- Dataset 2

```{r echo = TRUE, eval=FALSE, eval=FALSE, cache = FALSE, message=FALSE}
lev <- c("M", "B")

set.seed(27)
model_2 <- gafs(x = bc_data_2[, -c(1, 2)], y = as.factor(bc_data_2$diagnosis),
                   iters = 10, # generations of algorithm
                   popSize = 5, # population size for each generation
                   levels = lev,
                   gafsControl = ga_ctrl)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_2.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
model_2

plot(model_2)

bc_data_2_ga <- bc_data_2[, c(2, which(colnames(bc_data_2) %in% model_2$ga$final))]
```


- Dataset 3

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
lev <- c("R", "N")

set.seed(27)
model_3 <- gafs(x = bc_data_3[, -1], y = as.factor(bc_data_3$outcome),
                   iters = 10, # generations of algorithm
                   popSize = 5, # population size for each generation
                   levels = lev,
                   gafsControl = ga_ctrl)
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_3.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
model_3

plot(model_3)

bc_data_3_ga <- bc_data_3[, c(1, which(colnames(bc_data_3) %in% model_3$ga$final))]
```


# Model comparison

Now I can compare models with the different feature subsets.

For a more detailed description see [here](https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post) and [here](https://shiring.github.io/machine_learning/2016/12/02/flu_outcome_ML_2_post).


## All features

- Dataset 1

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_all_index <- createDataPartition(bc_data$classes, p = 0.7, list=FALSE)
bc_data_all_train_data <- bc_data[bc_data_all_index, ]
bc_data_all_test_data  <- bc_data[-bc_data_all_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_all <- train(classes ~ .,
                           data = bc_data_all_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_all.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_all, bc_data_all_test_data[, -1]), bc_data_all_test_data$classes)
```


- Dataset 2

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_2_all_index <- createDataPartition(bc_data_2$diagnosis, p = 0.7, list=FALSE)
bc_data_2_all_train_data <- bc_data_2[bc_data_2_all_index, -1]
bc_data_2_all_test_data  <- bc_data_2[-bc_data_2_all_index, -1]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_2_all <- train(diagnosis ~ .,
                           data = bc_data_2_all_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_2_all.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_2_all, bc_data_2_all_test_data[, -1]), bc_data_2_all_test_data$diagnosis)
```


- Dataset 3

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_3_all_index <- createDataPartition(bc_data_3$outcome, p = 0.7, list=FALSE)
bc_data_3_all_train_data <- bc_data_3[bc_data_3_all_index, ]
bc_data_3_all_test_data  <- bc_data_3[-bc_data_3_all_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_3_all <- train(outcome ~ .,
                           data = bc_data_3_all_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_3_all.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_3_all, bc_data_3_all_test_data[, -1]), bc_data_3_all_test_data$outcome)
```


## Selected features

### Dataset 1

```{r echo = TRUE, cache = FALSE, message=FALSE,fig.width = 5, fig.height = 5, fig.align = "center"}
library(gplots)

venn_list <- list(cor = colnames(bc_data_cor)[-1],
                  rfe = colnames(bc_data_rfe)[-1],
                  ga = colnames(bc_data_ga)[-1])

venn <- venn(venn_list)

venn
```


- Correlation

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_cor_index <- createDataPartition(bc_data_cor$classes, p = 0.7, list=FALSE)
bc_data_cor_train_data <- bc_data_cor[bc_data_cor_index, ]
bc_data_cor_test_data  <- bc_data_cor[-bc_data_cor_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_cor <- train(classes ~ .,
                 data = bc_data_cor_train_data,
                 method = "rf",
                 preProcess = c("scale", "center"),
                 trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_cor.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_cor, bc_data_cor_test_data[, -1]), bc_data_cor_test_data$classes)
```


- RFE

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_rfe_index <- createDataPartition(bc_data_rfe$classes, p = 0.7, list=FALSE)
bc_data_rfe_train_data <- bc_data_rfe[bc_data_rfe_index, ]
bc_data_rfe_test_data  <- bc_data_rfe[-bc_data_rfe_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_rfe <- train(classes ~ .,
                           data = bc_data_rfe_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_rfe.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_rfe, bc_data_rfe_test_data[, -1]), bc_data_rfe_test_data$classes)
```


- GA

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_ga_index <- createDataPartition(bc_data_ga$classes, p = 0.7, list=FALSE)
bc_data_ga_train_data <- bc_data_ga[bc_data_ga_index, ]
bc_data_ga_test_data  <- bc_data_ga[-bc_data_ga_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_ga <- train(classes ~ .,
                           data = bc_data_ga_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_ga.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_ga, bc_data_ga_test_data[, -1]), bc_data_ga_test_data$classes)
```


### Dataset 2

```{r echo = TRUE, cache = FALSE, message=FALSE,fig.width = 5, fig.height = 5, fig.align = "center"}
venn_list <- list(cor = colnames(bc_data_2_cor)[-c(1, 2)],
                  rfe = colnames(bc_data_2_rfe)[-c(1, 2)],
                  ga = colnames(bc_data_2_ga)[-c(1, 2)])

venn <- venn(venn_list)

venn
```

- Correlation

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_2_cor_index <- createDataPartition(bc_data_2_cor$diagnosis, p = 0.7, list=FALSE)
bc_data_2_cor_train_data <- bc_data_2_cor[bc_data_2_cor_index, -1]
bc_data_2_cor_test_data  <- bc_data_2_cor[-bc_data_2_cor_index, -1]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_2_cor <- train(diagnosis ~ .,
                           data = bc_data_2_cor_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_2_cor.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_2_cor, bc_data_2_cor_test_data[, -1]), bc_data_2_cor_test_data$diagnosis)
```


- RFE

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_2_rfe_index <- createDataPartition(bc_data_2_rfe$diagnosis, p = 0.7, list=FALSE)
bc_data_2_rfe_train_data <- bc_data_2_rfe[bc_data_2_rfe_index, ]
bc_data_2_rfe_test_data  <- bc_data_2_rfe[-bc_data_2_rfe_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_2_rfe <- train(diagnosis ~ .,
                           data = bc_data_2_rfe_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_2_rfe.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_2_rfe, bc_data_2_rfe_test_data[, -1]), bc_data_2_rfe_test_data$diagnosis)
```


- GA

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_2_ga_index <- createDataPartition(bc_data_2_ga$diagnosis, p = 0.7, list=FALSE)
bc_data_2_ga_train_data <- bc_data_2_ga[bc_data_2_ga_index, ]
bc_data_2_ga_test_data  <- bc_data_2_ga[-bc_data_2_ga_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_2_ga <- train(diagnosis ~ .,
                          data = bc_data_2_ga_train_data,
                          method = "rf",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_2_ga.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_2_ga, bc_data_2_ga_test_data[, -1]), bc_data_2_ga_test_data$diagnosis)
```


### Dataset 3

```{r echo = TRUE, cache = FALSE, message=FALSE,fig.width = 5, fig.height = 5, fig.align = "center"}
venn_list <- list(cor = colnames(bc_data_3_cor)[-1],
                  rfe = colnames(bc_data_3_rfe)[-1],
                  ga = colnames(bc_data_3_ga)[-1])

venn <- venn(venn_list)

venn
```

- Correlation

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_3_cor_index <- createDataPartition(bc_data_3_cor$outcome, p = 0.7, list=FALSE)
bc_data_3_cor_train_data <- bc_data_3_cor[bc_data_3_cor_index, ]
bc_data_3_cor_test_data  <- bc_data_3_cor[-bc_data_3_cor_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_3_cor <- train(outcome ~ .,
                           data = bc_data_3_cor_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_3_cor.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_3_cor, bc_data_3_cor_test_data[, -1]), bc_data_3_cor_test_data$outcome)
```


- RFE

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_3_rfe_index <- createDataPartition(bc_data_3_rfe$outcome, p = 0.7, list=FALSE)
bc_data_3_rfe_train_data <- bc_data_3_rfe[bc_data_3_rfe_index, ]
bc_data_3_rfe_test_data  <- bc_data_3_rfe[-bc_data_3_rfe_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_3_rfe <- train(outcome ~ .,
                           data = bc_data_3_rfe_train_data,
                           method = "rf",
                           preProcess = c("scale", "center"),
                           trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_3_rfe.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_3_rfe, bc_data_3_rfe_test_data[, -1]), bc_data_3_rfe_test_data$outcome)
```


- GA

```{r echo = TRUE, cache = FALSE, message=FALSE}
set.seed(27)
bc_data_3_ga_index <- createDataPartition(bc_data_3_ga$outcome, p = 0.7, list=FALSE)
bc_data_3_ga_train_data <- bc_data_3_ga[bc_data_3_ga_index, ]
bc_data_3_ga_test_data  <- bc_data_3_ga[-bc_data_3_ga_index, ]
```

```{r echo = TRUE, eval=FALSE, cache = FALSE, message=FALSE}
set.seed(27)
model_bc_data_3_ga <- train(outcome ~ .,
                          data = bc_data_3_ga_train_data,
                          method = "rf",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = FALSE))
```

```{r echo = FALSE, cache = FALSE, message=FALSE}
load("model_bc_data_3_ga.RData")
```

```{r echo = TRUE, cache = FALSE, message=FALSE}
confusionMatrix(predict(model_bc_data_3_ga, bc_data_3_ga_test_data[, -1]), bc_data_3_ga_test_data$outcome)
```

------------------

<br>

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4, fig.align="center", cache=FALSE}
sessionInfo()
```
